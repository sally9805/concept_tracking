@@773411

txt // obvious than that of attention , approval , affection , or even submissiveness . Money is not the only token . In education , for example , the individual behaves in part because of the marks , grades , and diplomas which he has received . These are not so readily exchanged for primary reinforcement as money , but the possibility of exchange is there . Educational tokens form a series in which one may be exchanged for the next , and the commercial or prestige value of the final token , the diploma , is usually clear . As a rule , prizes , medals , and scholarships for high marks or specialized skills or achievements are not explicitly paired with primary reinforccrs , but the clear-cut physical dimensions of such awards are an advantage in arranging contingencies . Usually the ultimate reinforcement is similar to that of prestige or esteem . It is easy to forget the origins of the generalized reinforcers and to regard them as reinforcing in their own right . We speak of the " need for attention , approval , @ @ @ @ @ @ @ @ @ @ " and " the love of money " as if they were primary conditions of deprivation . But a capacity to be reinforced in this way could scarcely have evolved in the short time during which the required conditions have prevailed . Attention , affection , approval , and submission have presumably existed in human society for only a very brief period , as the process of evolution goes . Moreover , they do not represent fixed forms of stimulation , since they depend upon the idiosyncrasies of particular groups . Insofar as affection is mainly sexual , it may be related to a condition of primary deprivation which is to some extent independent of the personal history of the individual , but the " signs of affection " which become reinforcing because of their association with sexual contact or with other reinforcers can scarcely be reinforcing for genetic reasons . Tokens are of even more recent advent , and it is not often seriously suggested that the need for them is inherited . We can usually watch the process through which a child comes to be reinforced by money . @ @ @ @ @ @ @ @ @ @ be autonomous as the " need for approval , " and if we confined ourselves to the observed effectiveness of these generalized reinforcers , we should have as much reason for assuming an inherited // need for money as for attention , approval , affection , or domination . Eventually generalized reinforcers arc effective even though the Ir , in.art reinforcers upon which they are based no longer accompany them . We play games of skill for their own sake . We get attention or approval for its own sake . Affection is not always followed by a more explicit sexual reinforcement . The submissiveness of others is reinforcing even though we make no use of it . A miser may be so reinforced by money that he will starve rather than give it up . These observable facts must have their place in any theoretical or practical consideration . They do not mean that generalized reinforcers are anything more than the physical properties of the stimuli observed in each case or that there are any nonphysical entities which must be taken into account . WHY IS A REINEORCER REINFORCING ? @ @ @ @ @ @ @ @ @ @ is simply a rule for strengthening behavior . When we reinforce a response and observe a change in its frequency , we can easily report what has happened in objective terms . But in explaining why it has happened we are likely to resort to theory . Why does reinforcement reinforce ? One theory is that an organism repeats a response because it finds the consequences " pleasant " or " satisfying . " But in what sense is this an explanation within the framework of a natural science ? " Pleasant " or " satisfying " apparently do not refer to physical properties of reinforcing events . since the physical sciences use neither these terms nor any equivalents . ' I'he terms must refer to some effect upon the organism , but can we define this in such a way that it will be useful in accounting for reinforcement ? It is sometimes argued that a thing is pleasant if an organism approaches or maintains contact with it and unpleasant if the organism avoids it or cuts it short . There are many variations on this attempt to find an @ @ @ @ @ @ @ @ @ @ same criticism : the behavior specified may be merely another product of the reinforcing effect . To say that a stimulus is pleasant in the sense that an organism tends to approach or prolong it may be only another way of saving that the stimulus has reinforced the // behavior of approaching or prolonging . Instead of defining a reinforcing effect in terms of its effect upon behavior in general , we have simply specified familiar behavior which is almost inevitably reinforced and hence generally available as an indicator of reinforcing power . It we then go on to say that a stimulus is reinforcing because it is pleasant , what purports to be an explanation in terms of two effects is in reality a redundant description of one . An alternative approach is to define " pleasant " and " unpleasant " ( or " satisfying " and " annoying " ) by asking the subject how he " feels " about certain events . This assumes that reinforcement has two effects ? it strengthens behavior and generates " feelings " ? and that one is a function of the @ @ @ @ @ @ @ @ @ @ other direction . When a man reports that an event is pleasant , he may be merely reporting that it is the sort of event which reinforces him or toward which he finds himself tending to move because it has reinforced such movement . We shall see in Chapter XVII that one could probably not acquire verbal responses with respect to pleasantness as a purely private fact unless something like this were so . In any case , the subject himself is not at an especially good point of vantage for making such observations . " Subjective judgments " of the pleasantness or satisfaction provided by stimuli are usually unreliable and inconsistent . As the doctrine of the unconscious has emphasized , we may not be able to report at all upon events which can be shown to be reinforcing to us or we may make a report which is in direct conflict with objective observations ; we may report as unpleasant a type of event which can be shown to be reinforcing . Examples of this anomaly range from masochism to martyrdom . It is sometimes argued that reinforcement is @ @ @ @ @ @ @ @ @ @ at least is a collateral effect which need not be confused with reinforcement itself . It is obvious that deprivation is important in operant conditioning . We used a hungry pigeon in our experiment , and we could not have demonstrated operant conditioning otherwise . The hungrier the bird , the oftener it responds as the result of reinforcement . But in spite of this connection it is not true that reinforcement always reduces deprivation . Conditioning may occur before any substantial change can take place // in the deprivation measured in other ways . All we can say is that the type of event which reduces deprivation is also reinforcing . The connection between reinforcement and satiation must be sought in the process of evolution . We can scarcely overlook the great biological significance of the primary reinforcers . Food , water , and sexual contact , as well as escape from injurious conditions ( Chapter XI ) , are obviously connected with the well-being of the organism . An individual who is readily reinforced by such events will acquire highly efficient behavior . It is also biologically advantageous if @ @ @ @ @ @ @ @ @ @ to occur in an appropriate state of deprivation . Thus it is important , not only that any behavior which leads to the receipt of food should become an important part of a repertoire , but that this behavior should be particularly strong when the organism is hungry . These two advantages are presumably responsible for the fact that an organism can be reinforced in specific ways and that the result will be observed in relevant conditions of deprivation . Some forms of stimulation are positively reinforcing although they do not appear to elicit behavior having biological significance . A baby is reinforced , not only by food , but by the tinkle of a bell or the sparkle of a bright object . Behavior which is consistently followed by such stimuli shows an increased probability . It is difficult , if not impossible , to trace these reinforcing effects to a history of conditioning . Later we may find the same individual being reinforced by an orchestra or a colorful spectacle . Here it is more difficult to make sure that the reinforcing effect is not conditioned . However , @ @ @ @ @ @ @ @ @ @ by any feedback from the environment would be biologically advantageous , since it would prepare the organism to manipulate the environment successfully before a given state of deprivation developed . When the organism generates a tactual feed-back , as in feeling the texture of a piece of cloth or the surface of a piece of sculpture , the conditioning is commonly regarded as resulting from sexual reinforcement , even when the area stimulated is not primarily sexual in function . It is tempting to suppose that other forms of stimulation produced by behavior are similarly related to biologically important events . // When the environment changes , a capacity to be reinforced by a given event may have a biological disadvantage . Sugar is highly reinforcing to most members of the human species , as the ubiquitous candy counter shows . Its effect in this respect far exceeds current biological requirements . This was not true before sugar had been grown and refined on an extensive scale . Until a few hundred years ago , the strong reinforcing effect of sugar must have been a biological advantage . The environment has @ @ @ @ @ @ @ @ @ @ not followed suit . Sex provides another example . There is no longer a biological advantage in the great reinforcing effect of sexual contact , but we need not go back many hundreds of years to find conditions of famine and pestilence under which the power of sexual reinforcement offered a decisive advantage . . A biological explanation of reinforcing power is perhaps as far as we can go in saving why an event is reinforcing . Such an explanation is probably of little help in a functional analysis , for it does not provide us with any way of identifving a reinforcing stimulus as such before we have tested its reinforcing power upon a given organism . We must therefore be content with a survey in terms of the effects of stimuli upon behavior . ACCIDENTAL CONTINGENCIES AND " SUPERSTITIOUS " BEHAVIOR It has been argued that Thorndike 's experiment is not typical of the learning process because the cat can not " see the connection " between moving a latch and escaping from a box . But seeing a connection is not essential in operant conditioning . Both during @ @ @ @ @ @ @ @ @ @ often talks about his behavior in relation to his environment ( Chapter XVII ) . His reports may be useful in a scientific account , and his reaction to his own behavior may even be an important link in certain complex processes . But such reports or reactions are not required in the simple process of operant conditioning . This is evident in the fact that one may not be able to describe a contingency which has clearly had an effect . Nor need there be any permanent connection between a response and its reinforcement . We made the receipt of food contingent upon // t lic response of our pigeon by arranging a mechanical and electrical connection . Outside the laboratory various physical systems are responsible for contingencies between behavior and its consequences . But these need not , and usually do not , affect the organism in any other way . So far as the organism is concerned , the only important property of the contingency is temporal . The reinforcer simply follows the response . How this is brought about does not matter . We must assume that @ @ @ @ @ @ @ @ @ @ it necessarily coincides with some behavior . We have also seen that a single reinforcement may have a substantial effect . If there is only an accidental connection between the response and the appearance of a reinforcer , the behavior is called " superstitious . " We may demonstrate this in the pigeon by accumulating the effect of several accidental contingencies . Suppose we give a pigeon a small amount of food every fifteen seconds regardless of what it is doing . When food is first given , the pigeon will be behaving in some way ? if only standing still ? and conditioning will take place . It is then more probable that the same behavior will be in progress when food is given again . If this proves to be the case , the " operant " will be further strengthened . If not , some other behavior will be strengthened . Eventually a given bit of behavior reaches a frequency at which is often reinforced . It then becomes a permanent part of the repertoire of the bird , even though the food has been given by a @ @ @ @ @ @ @ @ @ @ Conspicuous responses which have been established in this wavy include turning sharply to one side , hopping from one foot to the other and hack , bowing and scraping , turning around , strutting , and raising the head . The topography of the behavior may continue to drift with further reinforcements , since slight modifications in the form of response may coincide with the receipt of food . In producing superstitious behavior , the intervals at which food is given are important . At sixty seconds the effect of one reinforcement is largely lost before another can occur , and other behavior is more likely to appear . Superstitious behavior is therefore less likely to emerge , though it may do so if the experiment is carried on for a long time . At fifteen seconds the effect is usually almost immediate . // When a superstitious response has once been established , it will survive even when reinforced only infrequently . The pigeon is not exceptionally gullible . Human behavior is also heavily superstitious . Only a small part of the behavior strengthened by accidental contingencies develops into the @ @ @ @ @ @ @ @ @ @ the same principle is at work . Suppose we find a ten-dollar bill while walking through the park ( and suppose this is an event which has a considerable reinforcing effect ) . Whatever we were doing , or had just been doing , at the moment we found the bill must be assumed to be reinforced . It would be difficult to prove this in a rigorous way , of course , but it is probable that we shall be more likely to go walking again , particularly in the same or a similar park , that we shall be slightly more likely to keep our eyes cast downward precisely as we did when we saw the money , and so on . This behavior will vary with any state of deprivation to which money is relevant . We should not call it superstitious , but it is generated by a contingency which is only rarely " functional . " Some contingencies which produce superstitious behavior are not entirely accidental . A response is sometimes likely to be followed by a consequence which it nevertheless does not " produce . @ @ @ @ @ @ @ @ @ @ is reinforcing when removed ( Chapter XI ) . The termination of a brief stimulus of this sort may occur at just the right time to reinforce the behavior generated by its onset . The aversive stimulus appears and the organism becomes active ; the stimulus terminates , and this reinforces some part of the behavior . Certain illnesses , lamenesses , and allergic reactions are of such duration that any measure taken to " cure " them is likely to be reinforced when the condition clears up . The measure need not actually be responsible for the cure . The elaborate rituals of nonscientific medicine appear to be explained by this characteristic of many forms of illness . In superstitious operant behavior , as in the superstitious conditioned reflexes discussed in Chapter IV , the process of conditioning has miscarried . Conditioning offers tremendous advantages in equipping the organism with behavior which is effective in a novel environment , but there appears to be no way of preventing the acquisition of OPERANT BE1HAViOR 8 % non-advantageous behavior through accident . Curiously , this difficultv must have increased as the process @ @ @ @ @ @ @ @ @ @ If , for example , three reinforcements were always required in order to change the probability of a response , superstitious behavior would be unlikely . It is only because organisms have reached the point at which a single contingency makes a substantial change that they are vulnerable to coincidences . Superstitious rituals ire human society usually involve verbal formulae and are transmitted as part of the culture . To this extent they differ from the simple effect of accidental operant reinforcement . lint they must have had their origin in the same process , and they are probably sustained by occasional contingencies which follow the same pattern . COALS , PURPOSES , AND OTHER FINAL CAUSES It is not correct to say that operant reinforcement " strengthens the response which precedes it . " The response has already occurred and can not be changed . What is changed is the future probability of responses in the same class . It is the operant as a class of behavior , rattler than the response as a particular instance , which is conditioned . There is , therefore , no violation of @ @ @ @ @ @ @ @ @ @ causes . " But this principle is violated when it is asserted that behavior is under the control of an " incentive " or " goal " which the organism has not yet achieved or a " purpose " which it has not yet fulfilled . Statements which use such words as " incentive " or " purpose " are usually reducible to statements about operant conditioning , and only a slight change is required to bring them within the framework of a natural science . Instead of saying that a man behaves because of the consequences which are to follow his behavior , we simply say that he behaves because of the consequences which have followed similar behavior in the past . This is , of course , the Law of Effect or operant conditioning . It is sometimes argued that a response is not fully described until i ! s purpose is referred to as a current property . But what is meant by " describe " ? If we observe someone walking down the street , we may report this event in the language of physical science . @ @ @ @ @ @ @ @ @ @ to mail a letter , " have we said anything which was not included in our first report ? Evidently so , since a man may walk down the street " for many purposes " and in the same physical way in each case . But the distinction which needs to be made is not between instances of behavior ; it is between the variables of which behavior is a function . Purpose is not a property of the behavior itself ; it is a way of referring to controlling variables . If we make our report after we have seen our subject mail his letter and turn back , we attribute " purpose " to him from the event which brought the behavior of walking down the street to an end . This event " gives meaning " to his performance , not by amplifying a description of the behavior as such , but by indicating an independent variable of which it may have been a function . We can not see his " purpose " before seeing that he mails a letter , unless we have observed similar behavior @ @ @ @ @ @ @ @ @ @ , we use the term simply to predict that he will mail a letter upon this occasion . Nor can our subject see his own purpose without reference to similar events . If we ask him why he is going down the street or what his purpose is and he says , " I am going to mail a letter , " we have not learned anything new about his behavior but only about some of its possible causes . The subject himself , of course , may be in an advantageous position in describing these variables because he has had an extended contact with his own behavior for many years . But his statement is not therefore in a different class from similar statements made by others who have observed his behavior upon fewer occasions . As we shall see in Chapter XVII , he is simply making a plausible prediction in terms of his experiences with himself . Moreover , he may be wrong . He may report that he is " going to mail a letter , " and he may indeed carry an unmailed letter in his @ @ @ @ @ @ @ @ @ @ street , but we may still be able to show that his behavior is primarily determined by the fact that upon past occasions he has encountered someone who is important to him upon just such a walk . He may not be " aware of this purpose " in the sense of being able to say that his behavior is strong for this reason . The fact that operant behavior seems to be " directed toward the // future " is misleading . Consider , for example , the case of " looking for something . " In what sense is the " something " which has not yet been found relevant to the behavior ? Suppose we condition a pigeon to peck a spot on the wall of a box and then , when the operant is well established , remove the spot . The bird now goes to the usual place along the wall . It raises its head , cocks its eye in the usual direction , and may even emit a weak peck in the usual place . Before extinction is very far advanced , it returns @ @ @ @ @ @ @ @ @ @ . Must we say that the pigeon is " looking for the spot " ? Must we take the " looked for " spot into account in explaining the behavior ? It is not difficult to interpret this example in terms of operant reinforcement . Since visual stimulation from the spot has usually preceded the receipt of food , the spot has become a conditioned reinforcer . It strengthens the behavior of looking in given directions from different positions . Although we have undertaken to condition only the pecking response , we have in fact strengthened many different kinds of precurrent behavior which bring the bird into positions from which it sees the spot and pecks it . These responses continue to appear , even though we have removed the spot , until extinction occurs . The spot which is " being looked for " is the spot which has occurred in the past as the immediate reinforcement of the behavior of looking . In general , looking for something consists of emitting responses which in the past have produced " something " as a consequence . The same interpretation applies @ @ @ @ @ @ @ @ @ @ about a room opening drawers , looking under magazines , and so on , we may describe his behavior in fully objective terms : " Now he is in a certain part of the room ; he has grasped a book between the thumb and forefinger of his right hand ; he is lifting the book and bending his head so that any object under the book can be seen . " We may also " interpret " his behavior or " read a meaning into it " by saying that " he is looking for something " or , more specifically , that " he is looking for his glasses . " What we have added is not a further description of his behavior but an inference about some of the variables responsible for it . There is no current goal , in90 THE ANALYSIS OF BEHAVIOR centive , purpose , or meaning to be taken into account . This is so even if we ask him what he is doing and he says , " I am looking for my glasses . " This is not a further description @ @ @ @ @ @ @ @ @ @ behavior is a function ; it is equivalent to " I have lost my glasses , " " I shall stop what I am doing when I find my glasses , " or " When I have done this in the past , I have found my glasses . " These translations may seem unnecessarily roundabout , but only because expressions involving goals and purposes are abbreviations . Very often we attribute purpose to behavior as another way of describing its biological adaptability . This issue has already been discussed , but one point may be added . In both operant conditioning and the evolutionary selection of behavioral characteristics , consequences alter future probability . Reflexes and other innate patterns of behavior evolve because they increase the chances of survival of the species . Operants grow strong because they are followed by important consequences in the life of the individual . Both processes raise the question of purpose for the same reason , and in both the appeal to a final cause may be rejected in the same way . A spider does not possess the elaborate behavioral repertoire with which @ @ @ @ @ @ @ @ @ @ to capture the food it needs to survive . It possesses this behavior because similar behavior on the part of spiders in the past has enabled them to capture the food they needed to survive . A series of events have been relevant to the behavior of web-making in its earlier evolutionary history . We are wrong in saying that we observe the " purpose " of the web when we observe similar events in the life of the individual . SHAPING AND MAINTAINING OPERANT BEHAVIOR Operant conditioning shapes behavior as a sculptor shapes a lump of clay . Although at some point the sculptor seems to have produced an entirely novel object , we can always follow the process back t o the original undifferentiated lump , and we can make the successive stages by which we return to this condition as small as we wish . At no point does anything emerge which is very different from what preceded it . The final product seems to have a special unity or integrity of design , but we can not find a point at which this suddenly appears . In @ @ @ @ @ @ @ @ @ @ appears full grown in the behavior of the organism . It is the result of a continuous shaping process . The pigeon experiment demonstrates this clearly . " Raising the head " is not a discrete unit of behavior . It does not come , so to speak , in a separate package . We reinforce only slightly exceptional values of the behavior observed while the pigeon is standing or moving about . We succeed in shifting the whole range of heights at which the head is held , but there is nothing which can be accurately de-91 // scribed as a new " response . " A response such as turning the latch in a problem box appears to be a more discrete unit , but only because the continuity with other behavior is more difficult to observe . In the pigeon , the response of pecking at a spot on the wall of the experimental box seems to differ from stretching the neck because no other behavior of the pigeon resembles it . If in reinforcing such a response we simply wait for it to occur ? and we @ @ @ @ @ @ @ @ @ @ ? the whole unit appears to emerge in its final form and to be strengthened as such . There may be no appreciable behavior which we could describe as " almost pecking the spot . " The continuous connection between such an operant and the general behavior of the bird can nevertheless easily be demonstrated . It is the basis of a practical procedure for setting up a complex response . To get the pigeon to peck the spot as quickly as possible we proceed as follows : We first give the bird food when it turns slightly in the direction of the spot from any part of the cage . This increases the frequency of such behavior . We then withhold reinforcement until a slight movement is made toward the spot . This again alters the general distribution of behavior without producing a new unit . We continue by reinforcing positions successively closer to the spot , then by reinforcing only when the head is moved slightly forward , and finally only when the beak actually makes contact with the spot . We may reach this final response in a @ @ @ @ @ @ @ @ @ @ to the situation and to the food tray , can usually be brought to respond in this way in two or three minutes . The original probability of the response in its final form is very low ; in some cases it may even be zero . In this way we can build complicated operants which would never appear in the repertoire of the organism otherwise . By reinforcing a series of successive approximations , we bring a rare response to a very high probability in a short time . This is an effective procedure because it recognizes and utilizes the continuous nature of a complex act . The total act of turning toward the spot from any point the box , walking toward it , raising the head , and striking the spot may seem to be a functionally coherent unit of behavior ; but it is constructed by a continual process of differential reinforcement from undifferentiated behavior , just as the // sculptor shapes his figure from a lump of clay . When we wait for a single complete instance , we reinforce a similar sequence but far less @ @ @ @ @ @ @ @ @ @ This account is inaccurate in one respect . We may detect a discontinuity between bringing the head close to the spot and pecking . The pecking movement usually emerges as an obviously preformed unit . There are two possible explanations . A mature pigeon will already have developed a well-defined pecking response which may emerge upon the present occasion . The history of this response might show a similar continuity if we could follow it . It is possible , however , that there is a genetic discontinuity , and that in a bird such as the pigeon the pecking response has a special strength and a special coherence as a form of species behavior . Vomiting and sneezing are human responses which probably have a similar genetic unity . Continuity with other behavior must be sought in the evolutionary process . But these genetic units are rare , at least in the vertebrates . The behavior with which we are usually concerned , from either a theoretical or practical point of view , is continuously modified from a basic material which is largely undifferentiated . Through the reinforcement of slightly @ @ @ @ @ @ @ @ @ @ raise himself , to stand , to walk , to grasp objects , and to move them about . Later on , through the same process , he learns to talk , to sing , to dance , to play games ? in short , to exhibit the enormous repertoire characteristic of the normal adult . When we survey behavior in these later stages , we find it convenient to distinguish between various operants which differ from each other in topography and produce different consequences . In this way behavior is broken into parts to facilitate analysis . These parts are the units which we count and whose frequencies play an important role in arriving at laws of behavior . They are the " acts " into which , in the vocabulary of the layman , behavior is divided . But if we are to account for many of its quantitative properties , the ultimately continuous nature of behavior must not be forgotten . Neglect of this characteristic has been responsible for several difTic ult problems in behavior theory . An example is the effect sometimes spoken of as " response @ @ @ @ @ @ @ @ @ @ in94 THE ANALYSIS OF BEHAVIOR duction . " In reinforcing one operant we often produce a noticeable increase in the strength of another . Training in one area of skilled behavior may improve performance in another . Success in one field of activity may increase the tendency to be active in other fields . By arranging optimal reinforcing contingencies in the clinic or institution , the psychotherapist strengthens behavior in the world at large . But how is this possible ? What is the " transfer " which appears to strengthen behavior without reinforcing it directly ? This is a good example of a pseudo problem . We divide behavior into hard and fast units and are then surprised to find that the organism disregards the boundaries we have set . It is difficult to conceive of two responses which do not have something in common . Sometimes the same muscular system is used . The effect of a reinforcement may reflect this fact rather than our arbitrary practice of calling the responses separate units . Again , when we reinforce the final response in a sequence containing many precurrent members @ @ @ @ @ @ @ @ @ @ precurrent members . Our skill in manipulating tools and instruments transfers from one field of reinforcement to another . The traditional explanation of transfer asserts that the second response is strengthened only insofar as the responses " possess identical elements . " This is an effort to maintain the notion of a unit of response . A more useful way of putting it is to say that the elements are strengthened wherever they occur . This leads us to identify the element rather than the response as the unit of behavior . It is a sort of behavioral atom , which may never appear by itself upon any single occasion but is the essential ingredient or component of all observed instances . The reinforcement of a response increases the probability of all responses containing the same elements . Verbal behavior supplies especially good examples of the need to consider these atoms . An enormous number of verbal responses are executed by the same musculature . They are responses , therefore , which are presumably composed of a fairly small number of identical elements . This is not usually recognized in the @ @ @ @ @ @ @ @ @ @ separate units ? for example , the " words " of the grammarian . A rigorous analysis shows that the word is by no means the functional unit . Larger complexes of words ? idioms , // phrases , or memorized passages ? may vary together under the control of a single variable . On the other hand , we may observe the separate functional control of " atoms " at least as small as the separate speech sounds . We have to recognize these small units in order to account for such distorted verbal responses as spoonerisms and certain verbal slips , as well as the stylistic devices of alliteration , assonance , rhyme , and rhythm . We lack adequate tools to deal with the continuity of behavior or with the interaction among operants attributable to common atomic units . The operant represents a valid level of analysis , however , because the properties which define a response are observable data . A given set of properties may be given a functional unity . Although methods must eventually be developed which will not emphasize units at this level , @ @ @ @ @ @ @ @ @ @ dynamic properties of behavior . DIFFERENTIAL REINFORCEMENT Although operant reinforcement is always a matter of selecting certain magnitudes of response as against others , we may distinguish between producing a relatively complete new unit and making slight changes in the direction of greater effectiveness in an existing unit . In the first case , we are interested in how behavior is acquired ; in the second , in how it is refined . It is the difference between " knowing how to do something " and " doing it well . " The latter is the field of skill . The contingency which improves skill is the differential reinforcement of responses possessing special properties . It may example , certain responses must release the ball from the fingers at the moment of its greatest forward speed . These responses are differentially reinforced by the fact that , when so released , the ball covers a considerable distance . Other instances in which the release comes before or after the proper moment are not so reinforced . We are likely to forget how complex an aci this is and how much differential @ @ @ @ @ @ @ @ @ @ properly timed sequence . In games , crafts , // and certain artistic performances extremely fine differences in the execution of behavior make important differences in the consequences . ( The consequences at issue are generally the conditioned reinforcers summarized in Chapter V. Primary reinforcers are seldom involved . The negative reinforcers to be considered in Chapter XI also are important . For example , the consequences which are effective in conditioning postural responses in locomotion or the maintenance of an upright position are largely the avoidance of falls , bumps , and awkward or painful postures . ) The reinforcement which develops skill must be immediate . Otherwise , the precision of the differential effect is lost . In many practical areas skilled behavior is encouraged by arranging a quick report of accomplishment . In rifle practice , for example , extremely small-scale properties of response are differentially reinforced by a hit or a miss . Properties of this magnitude can be selected only if the differential reinforcement is immediate . But even when a hit can be seen by the rifleman , the report is delayed by the time @ @ @ @ @ @ @ @ @ @ this gap is bridged by conditioned reinforcement from the " feel " of the shot . The rifleman eventually " knows " before the target is hit whether the shot was good or bad . His own behavior generates a stimulating feed-back , certain forms of which are followed by hits , others by misses . The more immediate problem is to shoot in such a way as to generate the " feel " followed by a hit . In more vigorous enterprises the feed-back is clearer . Good form in bowling , for example , is reinforced by feed-back from the bowler 's body . This does not mean that the rifleman will continue to shoot well , or the bowler to bowl well , even though he receives no report of the effect upon the target or pins . The report is needed to maintain the conditioned reinforcing power of the feed-back . If the differential contingencies change , the topography of behavior changes with them . Even the very common responses which enable us to walk upright continue to be modified by the environment . When we walk @ @ @ @ @ @ @ @ @ @ special set of contingencies prevails in maintaining our orientation in the gravitational field . The new differential reinforcement sets up " sea legs . " At the end of the voyage the old contingencies work a reverse change . ConSHAPING // tingencies of reinforcement which are arranged by society are especially likely to shift . Verbal behavior supplies many good examples . In the nursery , crude vocal responses are successful ; the indulgent parent may even reinforce " baby talk " into adolescent or adult years . But eventually , verbal behavior is successful only when it generates suitable behavior in the average listener ; therefore , the form of the behavior comes to correspond more and more closely to the standards of a given community . When we move from one community to another , the topography of our behavior may change . Some differential reinforcements make a response more or less intense or forceful without appreciably altering its topography . Certain natural contingencies in the environment lead us to push or lift harder to move objects , to pull harder to break objects apart , to jump harder @ @ @ @ @ @ @ @ @ @ In calling to someone at a distance or in talking to a deaf person , our verbal behavior is reinforced only when it reaches a certain level of loudness . Tests of strength and other competitive games supply examples of these differential contingencies . When a heavy ball is thrown beyond a certain mark , when a horizontal bar is cleared in vaulting or in jumping , when a ball is batted over the fence ( and when , as a result , a record is broken or a match or game won ) , differential reinforcement is at work . It may to some extent change the topography of the behavior and produce " good form , " but it has an important effect upon the mere force with which the behavior is executed . We use differential reinforcement to shape and intensify the behavior of others in what may be spoken of , as we shall see in Chapt cr XX , as deliberate control . The effect may also be wholly unintentional . The mother who complains that her three-year-old child whines and cries for attention in an @ @ @ @ @ @ @ @ @ @ are responsible . If she is busy with other matters , she is likely not to respond to a call or request made in a quiet tone of voice . When the child raises his voice , she replies . This is differential reinforcement . The average intensity of the child 's vocal behavior rises . When the mother has adapted to the new level , again only the louder instances are reinforced . Further differentiation in the direction of loud responses follows . The child 's voice may also // vary in intonation . What we call " whining " may be thought of as speaking with a small admixture of crying . Such speech is more likely to secure an effect and is therefore differentially strengthened . In fact , what we call annoying behavior in general is just that behavior which is especially effective in arousing another person to action . Differential reinforcement supplied by a preoccupied or negligent parent is very close to the procedure we should adopt if we were given the task of conditioning a child to be annoying . THE MAINTENANCE OF BEHAVIOR One @ @ @ @ @ @ @ @ @ @ " operant conditioning " is that traditionally it has been confined to the process of learning how to do something . In trial-and-error learning , for example , the organism learns how to get out of a box or how to find its way through a maze . It is easy to see why the acquisition of behavior should be emphasized . Early devices for the study of learning did not reveal the basic process directly . The effect of operant reinforcement is most conspicuous when there is a gross change in behavior . Such a chance occurs when an organism learns how to make a response which it did not or could not make before . A more sensitive measure , however , enables us to deal with cases in which the acquisition of behavior is of minor importance . Operant conditioning continues to be effective even when there is no further change which can be spoken of as acquisition or even as improvement in skill . Behavior continues to have consequences and these continue to be important . If consequences are not forthcoming , extinction occurs . When we @ @ @ @ @ @ @ @ @ @ the complexity of its everyday life , we need to be constantly alert to the prevailing reinforcements which maintain its behavior . We may , indeed , have little interest in how that behavior was first acquired . Our concern is only with its present probability of occurrence , which can be understood only through an examination of current contingencies of reinforcement . This is an aspect of reinforcement which is scarcely ever dealt with in classical treatments of learning . // INTERMITTENT REINFORCEMENT In general , behavior which acts upon the immediate physical en . vironment is consistently reinforced . We orient ourselves toward objects and approach , reach for , and seize them with a stable repertoire of responses which have uniform consequences arising from the optical and mechanical properties of nature . It is possible , of course , to disturb the uniformity . In a " house of mirrors " in an amusement park , or in a room designed to supply misleading cues to the vertical , well-established responses may fail to have their usual effects . But the fact that such conditions are so unusual @ @ @ @ @ @ @ @ @ @ the everyday world . A large part of behavior , however , is reinforced only intermittently . A given consequence may depend upon a series of events which are not easily predicted . We do not always win at cards or dice , because t h e contingencies are so remotely determined that we call them " chance . " We do not always find good ice or snow when we go skating or skiing . Contingencies which require the participation of people are especially likely to be uncertain . We do not always get a good meal in a particular restaurant because cooks are not always predictable . We do not always get an answer when we telephone a friend because the friend is not always at home . We do not always get a pen by reaching into our pocket because we have not always put it there . The reinforcements characteristic of industry and education arc almost always intermittent because it is not feasible to control behavior by reinforcing every response . As might be expected , behavior which is reinforced only intermittently often shows an intermediate frequency @ @ @ @ @ @ @ @ @ @ revealed some surprising complexities . Usually such behavior is remarkably stable and shows great resistance to extinction . An experiment has already been mentioned in which more than 10,000 responses appeared in the extinction curve of a pigeon which had been reinforced on a special schedule . Nothing of the sort is ever obtained after continuous reinforcement . Since this is a technique for " getting more responses out of an organism " in return for a given number of reinforcements , // it is widely used . Wages are paid in special ways and betting and gambling devices are designed to " pay off " on special schedules because of the relatively large return on the reinforcement in such a case . Approval , affection , and other personal favors are frequently intermittent , not only because the person supplying the reinforcement may behave in different ways at different times , but precisely because he may have found that such a schedule yields a more stable , persistent , and profitable return . It is important to distinguish between schedules which are arranged by a system outside the organism and @ @ @ @ @ @ @ @ @ @ example of the first is a schedule of reinforcement which is determined by a clock ? as when we reinforce a pigeon every five minutes , allowing all intervening responses to go unreinforced . An example of the second is a schedule in which a response is reinforced after a certain number of responses have been emitted ? as when we reinforce every fiftieth response the pigeon makes . The cases are similar in the sense that we reinforce intermittently in both , but subtle differences in the contingencies lead to very different results , often of great practical significance . Interval reinforcement . If we reinforce behavior at regular intervals , an organism such as a rat or pigeon will adjust with a nearly constant rate of responding , determined by the frequency of reinforcement . If we reinforce it every minute , the animal responds rapidly ; if every five minutes , much more slowly . A similar effect upon probability of response is characteristic of human behavior . How often we call a given number on the telephone will depend , other things being equal , upon how @ @ @ @ @ @ @ @ @ @ the same service , we are more likely to call the one which answers more often . We are less likely to see friends or acquaintances with whom we only occasionally have a good time , and we are less likely to write to a correspondent who seldom answers . The experimental results are precise enough to suggest that in general the organism gives back a certain number of responses for each response reinforced . We shall see , however , that the results of schedules of reinforcement are not always reducible to a simple equating of input with output . Since behavior which appears under interval reinforcement is cspeSHAPING // eially stable , it is useful in studying other variables and conditions . The size or amount of each reinforcement affects the rate ? more responses appearing in return for a larger reinforcement . Different kinds of reinforcers also yield different rates , and these may be used to rank reinforcers in the order of their effectiveness . The rate varies with the immediacy of the reinforcement : a slight delay between response and t he receipt of the reinforcer @ @ @ @ @ @ @ @ @ @ been studied under interval reinforcement will be discussed in later chapters . They include the degree of deprivation and the presence or absence of certain emotional circumstances . Optimal schedules of reinforcement are often of great practical importance . They are often discussed in connection with other variables which affect the rate . Reinforcing a man with fifty dollars at one time may not be so effective as reinforcing him with five dollars at ten different times during the same period . This is especially the case with primitive people where conditioned reinforcers have not been established to bridge the temporal span between a response and its ultimate consequence . There are also many subtle interactions betwccn schedules of reinforcement and levels of motivation , immediacy of reinforcement , and so on . If behavior continues to be reinforced at fixed intervals , another process intervenes . Since responses are never reinforced just after reinforcement , a change , to be described in Chapter VII , eventually tikes place in which the rate of responding is low for a short time after each reinforcement . The rate rises again when an @ @ @ @ @ @ @ @ @ @ not distinguish howl the interval at which it is reinforced . These changes in rate are not characteristic of the effect of wages in industry , which would otherwise appear to be an example of a fixed-interval schedule . The discrepancy is explained by the fact that other reinforcing systems arc used to maintain a given level of work , as we shall see in Chapter XXV . Docking a man for time absent guarantees his presence each day by establishing a time-card entry as a conditioned reinforcer . I lie aversive reinforcement ( Chapter XI ) supplied by a supervisor or boss is , however , the principal supplement to a fixed-interval wage . A low probability of response just after reinforcement is eliminated // with what is called variable-interval reinforcement . Instead of reinforcing a response every five minutes , for example , we reinforce every five minutes on the average , where the intervening interval may be as short as a few seconds or as long as , say , ten minutes . Reinforcement occasionally occurs just after the organism has been reinforced , and the organism therefore @ @ @ @ @ @ @ @ @ @ such a schedule is remarkably stable and uniform . Pigeons reinforced with food with a variable interval averaging five minutes between reinforcements have been observed to respond for as long as fifteen hours at a rate of from two to three responses per second without pausing longer than fifteen or twenty seconds during the whole period . It is usually very difficult to extinguish a response after such a schedule . Many sorts of social or personal reinforcement are supplied on what is essentially a variable-interval basis , and extraordinarily persistent behavior is sometimes set up . Ratio reinforcement . An entirely different result is obtained when the schedule of reinforcement depends upon the behavior of the organism itself ? when , for example , we reinforce every fiftieth response . This is reinforcement at a " fixed ratio " ? the ratio of reinforced to unreinforced responses . It is a common schedule in education , where the student is reinforced for completing a project or a paper or some other specific amount of work . It is essentially the basis of professional pay and of selling on commission . @ @ @ @ @ @ @ @ @ @ is a system of reinforcement which naturally recommends itself to employers because the cost of the labor required to produce a given result can be calculated in advance . Fixed-ratio reinforcement generates a very high rate of response provided the ratio is not too high . This should follow from the inputoutput relation alone . Any slight increase in rate increases the frequency of reinforcement with the result that the rate should rise still further . If no other factor intervened , the rate should reach the highest possible value . A limiting factor , which makes itself felt in industry , is simple fatigue . The high rate of responding and the long hours of work generated by this schedule can be dangerous to health . This is the main reason why piecework pay is usually strenuously opposed by organized labor . // Another objection to this type of schedule is based upon the possibility that as the rate rises , the reinforcing agency will move to a larger ratio . In the laboratory , after first reinforcing every tenth response and then every fiftieth , we may find it @ @ @ @ @ @ @ @ @ @ not have used this ratio in the beginning . In industry , the employee whose productivity has increased as the result of a piecework schedule may receive so large a weekly wage that the employer feels justified in increasing the number of units of work required for a given Unit of pay . Under ratios of reinforcement which can be sustained , the behavior eventually shows a very low probability just after reinforcement , as it does in the case of fixed-interval reinforcement . The effect is marked under high fixed ratios because the organism always has " a long way to go " before the next reinforcement . Wherever a piecework schedule is used ? in industry , education , salesmanship , or the professions ? low morale or low interest is most often observed just after a unit of work has been completed . When responding begins , the situation is improved by each response and the more the organism responds , the better the chances of reinforcement become . The result is a smooth gradient of acceleration as the organism responds more and more rapidly . The condition @ @ @ @ @ @ @ @ @ @ over-all mode of responding . It makes relatively poor use of the available time , and the higher rates of responding may be especially fatiguing . The laboratory study of ratio reinforcement has shown that for a given organism and a given measure of reinforcement there is a limiting ratio beyond which behavior can not he sustained . The result of exceeding this ratio is an extreme degree of extinction of the sort which we call abulia ( Chapter V ) . Long periods of inactivity begin to appear between separate ratio runs . This is not physical fatigue , as we may easily show by shifting to another schedule . It is often called " mental " fatigue , but this designation adds nothing to the observed fact that beyond a certain high ratio of reinforcement the organism simply has no behavior available . In both the laboratory study of ratio reinforcement and its practical application in everyday life , the first signs of strain imposed by too high a ratio are seen in these // breaks . Before a pigeon stops altogether ? in complete " abulia " ? @ @ @ @ @ @ @ @ @ @ reinforcement . In the same way , the student who has finished a term paper , perhaps in a burst of speed at the end of the gradient , finds it difficult to start work on a new assignment . Exhaustion can occur under ratio reinforcement because there is no self-regulating mechanism . In interval reinforcement , on the other hand , any tendency toward extinction is opposed by the fact that when the rate declines , the next reinforcement is received in return for fewer responses . The variable-interval schedule is also selfprotecting : an organism will stabilize its behavior at a given rate under any length of interval . We get rid of the pauses after reinforcement on a fixed-ratio schedule by adopting essentially the same practice as in variableinterval reinforcement : we simply vary the ratios over a considerable range around some mean value . Successive responses may be reinforced or many hundreds of unreinforced responses may intervene . The probability of reinforcement at any moment remains essentially constant and the organism adjusts by holding to a constant rate . This " variable-ratio reinforcement " is much more @ @ @ @ @ @ @ @ @ @ of responses . A pigeon may respond as rapidly as five times per second and maintain this rate for many hours . The efficacy of such schedules in generating high rates has long been known to the proprietors of gambling establishments . Slot machines , roulette wheels , dice cages , horse races , and so on pay off on a schedule of variable-ratio reinforcement . Each device has its own auxiliary reinforcements , but the schedule is the important characteristic . Winning depends upon placing a bet and in the long run upon the number of bets placed , but no particular payoff can be predicted . The ratio is varied by any one of several " random " systems . The pathological gambler exemplifies the result . Like the pigeon with its five responses per second for many hours , he is the victim of an unpredictable contingency of reinforcement . The long-term net gain or loss is almost irrelevant in accounting for the effectiveness of this schedule . // A combined schedule . It is fairly easy to combine ratio and interval reinforcement in a laboratory experiment so @ @ @ @ @ @ @ @ @ @ and by the number of unreinforced responses emitted . In such a case , if the organism is responding rapidly , it responds many times before being reinforced , but if it is responding slowly , , only a few responses occur before the next reinforcement . Such a schedule resembles either interval or ratio reinforcement , depending upon the values chosen in the combination , but there is some evidence that there is a middle ground in which neither schedule predominates and that the resulting behavior is unstable . Although this combined schedule may seem quite arbitrary , it is exemplified by many social situations where , as we shall see in Chapter XIX , the reinforcing agent may be affected by the level of t 1 is behavior reinforced . We can reinforce an organism only when responses are occurring at a specified rate . If we reinforce only when , say , the four preceding responses have occurred within two seconds , we generate a very high rate . This is maintained even when we reinforce only at varying intervals with a fairly long mean interval . The @ @ @ @ @ @ @ @ @ @ the same net rate of reinforcement . Reinforcing a low rate of responding at variable intervals has the opposite effect of generating a sustained low rate . These studies have yielded many facts , too detailed to be discussed here , which explain why a given schedule of reinforcement has the effect it has . The effects of a schedule are due to the contingencies which prevail at the moment of reinforcement under it . Such schedules are , in other words , simply rather inaccurate ways of reinforcing rates of responding . They are often the most convenient way of doing this , and this may explain their widespread use in the practical conlrol of behavior . But with proper instrumentation it should be possible to improve upon established practices in all these fields . Thus gambling devices could be " improved " ? from the point of view of 1he proprietor ? by introducing devices which would pay off on a variable-interval basis , but only when the rate of play is exceptionally high . The device would need to be more complex than the slot machine or roulette @ @ @ @ @ @ @ @ @ @ play . Schedules of pay in industry , salesmanship , and the professions , and the use of bonuses , incentive wages , and so on , could also be improved from the point of view of generating maximal productivity . Whether these improvements should be permitted is a matter to be discussed later . A schedule of reinforcement not only increases productivity , it also increases the interest , morale , and happiness of the worker . Any decision concerning a choice of schedules is complicated by this fact . In any event , we can act intelligently in this area only if we are in possession of clear-cut information regarding the nature and effect of the devices responsible for the maintenance of behavior in strength . We have much to gain from a close study of the results of experimental analyses . OPERANT DISCRIMINATION Operant conditioning may be described without mentioning ny stimulus which acts before the response is made . In reinforcing eck-stretching in the pigeon it was necessary to wait for the stretchng to occur ; we did not elicit it . When a baby puts his @ @ @ @ @ @ @ @ @ @ by the contact of hand lid mouth , but we can not find any stimulus which elicits the movetent and which is present every time it occurs . Stimuli are always sting upon an organism , but their functional connection with opernt behavior is not like that in the reflex . Operant behavior , in short , emitted , rather than elicited . It must have this property if the it ion of probability of response is to make sense . lost operant behavior , however , acquires important connections with the surrounding world . We may show how it does so in our igeon experiment by reinforcing neck-stretching when a signal light on and allowing it to be extinguished when the light is off . Eventtalla stretching occurs only when the light is on . We can then rnionstrate a stimulus-response connection which is roughly cornrabic to a conditioned or unconditioned reflex : the appearance of c light will be quickly followed by an upward movement of the 107 // head . But the relation is fundamentally quite different . It has a different history and different current properties . @ @ @ @ @ @ @ @ @ @ the light ) is the occasion upon which a response ( stretching the neck ) is followed by rein f orcement ( with food ) . We must specify all three terms . The effect upon the pigeon is that eventually the response is more likely to occur when the light is on . The process through which this comes about is called discrimination . Its importance in a theoretical analysis , as well as in the practical control of behavior , is obvious : when a discrimination has been established , we may alter the probability of a response instantly by presenting or removing the discriminative stimulus . Operant behavior almost necessarily comes under this kind of stimulus control , since only a few responses are automatically reinforced by the organism 's own body without respect to external circumstances . Reinforcement achieved by adjusting to a given environment almost always requires the sort of physical contact which we call stimulation . The environmental control has an obvious biological significance . If all behavior were equally likely to occur on all occasions , the result would be chaotic . It is @ @ @ @ @ @ @ @ @ @ likely to be reinforced . The three-term contingencies which produce discriminative operants are of many kinds . We develop the behavior with which we adjust to the spatial world because visual stimulation from an object is the occasion upon which certain responses of walking , reaching , and so on lead to particular tactual consequences . The visual field is the occasion for effective manipulatory action . The contingencies responsible for the behavior are generated by the relations between visual and tactual stimulation characteristic of physical objects . Other connections between the properties of objects supply other sorts of contingencies which lead to similar changes in behavior . For example , in an orchard in which red apples are sweet and all others sour , the behavior of picking and eating comes to be controlled by the redness of the stimulus . The social environment contains vast numbers of such contingencies . A smile is an occasion upon which social approach will meet with approval . A frown is an occasion upon which the same approach // will not meet with approval . Insofar as this is generally true , approach @ @ @ @ @ @ @ @ @ @ of the person approached . We use this fact when by smiling or frowning we control to some extent the behavior of those approaching us . The ringing of a telephone is an occasion upon which answering will be followed by hearing a voice . The young child may pick up and speak into the telephone at any time , but eventually he will do so only when it has been ringing . The verbal stimulus " Come to dinner " is an occasion upon which going to a table and sitting down is usually reinforced with food . The stimulus comes to be effective in increasing the probability of that behavior and is produced by the speaker because it does so . Bells , whistles , and traffic signals are other obvious occasions upon which certain actions are generally followed by certain consequences . Verbal behavior fits the pattern of the three-term contingency and supplies many illuminating examples . We learn to name objects by acquiring an enormous repertoire of responses each of which is appropriate to a given occasion . A chair is the occasion upon which the response @ @ @ @ @ @ @ @ @ @ cat is the occasion upon which the response " cat " is likely to be reinforced , and so on . When we read aloud , we respond to a series of visual stimuli with a series of corresponding vocal responses . The three-term contingency is evident in teaching a child to read , when a given response is reinforced with " right " or " wrong " according to the presence or absence of the appropriate visual stimulus . Many verbal responses are under the control of verbal discrimination stimuli . In memorizing the multiplication table , for example , the stimulus " 9 X 9 " is the occasion upon which the response " 81 " is appropriately reinforced , either by an instructor or by the successful outcome of a calculation . Historical " facts " and many other types f information fit the same formula . When the student writes an examination , he emits , insofar as it has become part of his repertoire , the behavior which is reinforced upon the special occasion established by the examination question . We use operant discrimination in two @ @ @ @ @ @ @ @ @ @ already become discriminative are manipulated in // order to change probabilities . We do this explicitly and almost con , tinuously when we direct constructive work , control the behavior of children , issue orders , and so on . We do it more subtly when we arrange stimuli whose effectiveness has not been specifically established for such purposes . In displaying merchandise in a large store the behavior of the customer is controlled through existing discriminative operants . The purchase of certain types of merchandise may be assumed to be strongly determined by conditions which commonly bring customers to the store . It is a mistake to exhibit this merchandise in the front of the store , since the customer will then buy and leave . Instead , goods are displayed which are more likely to be purchased " on the spur of the moment " rather than as the result of deprivations sufficient to bring the customer into the store . The display serves as a " reminder " in the sense of making the occasion optimal for the emission of weak behavior . In the second place , @ @ @ @ @ @ @ @ @ @ sure that a future stimulus will have a given effect when it appears . Education is largely a matter of establishing such discriminative repertoires , as we shall see in Chapter XXVI . We set up contingencies which generate behavior as the result of which children will look before crossing streets , will say " Thank you " upon the proper occasions , will give correct answers to questions about historical events , will operate machines in the proper manner , will buy books , attend concerts , plays , and moving pictures identified in certain ways , and so on . VOLUNTARY AND INVOLUNTARY BEHAVIOR The relation between the discriminative operant and its controlling stimulus is very different from elicitation . Stimulus and response occur in the same order as in the reflex , but this does not warrant the inclusion of both types in a single " stimulus-response " formula . The discriminative stimulus does not elicit a response , it simply alters a probability of occurrence . The relation is flexible and continuously graded . The response follows the stimulus in a more leisurely fashion , and it @ @ @ @ @ @ @ @ @ @ intensity // of the stimulus . This difference is at the root of the classical distinction between voluntary and involuntary behavior . In the early history of the reflex , an effort was made to distinguish between reflexes and the rest of the behavior of the organism . One difference frequently urged was that a reflex was innate , but the principle of conditioning made such a distinction trivial . It was also said that reflexes were different because they were unconscious . This did not mean that the individual could not report upon his own reflex behavior , but that the behavior made its appearance whether lie could do so or not . Reflex action might take place when a man was asleep or otherwise " unconscious . " As we shall see in Chapter XVII , this too is no longer considered a valid difference ; behavior which is clearly not reflex may occur under such circumstances . A third classical distinction held that reflexes were not only innate and unconscious , but " involuntary . " They were not " willed . " The evidence was not so @ @ @ @ @ @ @ @ @ @ could not be willed against . A certain part of the behavior of the organism can not , so to speak , be helped . We may not be able to keep from winking when something moves close to our eves . We may not be able to help flinching at gunfire or salivating at the taste of a lemon or ( through a conditioned reflex ) at the sight of a lemon . Before the discovery of the reflex such behavior was reconciled with a scheme of inner causation by postulating separate causes . It was attributed to seditious selves or foreign spirits temporarily invading the body . The involuntary sneeze , for example , revealed the presence of t he Devil . ( We still take the precaution of saying " God bless you " When someone sneezes . ) With the advent of the notion of the reflex he issue of controllability became less crucial . In the present analysis we can not distinguish between involuntary and voluntary behavior by raising the issue of who is in control . It does not matter whether behavior is due to @ @ @ @ @ @ @ @ @ @ all inner agents of whatever sort . Nor an we make the distinction on the basis of control or lack of control , nice we assume that no behavior is free . If we have no reason to disinguish between being able to do something and doing it , such expressions as " not being able to do something " or " not being able to // help doing something " must be interpreted in some other way . When all relevant variables have been arranged , an organism will or will not respond . If it does not , it can not . If it can , it will . To ask whether someone can turn a handspring is merely to ask whether there arc circumstances under which he will do so . A man who can avoid flinching at gunfire is a man who will not flinch under certain circumstances . A man who can hold still while a dentist works on his teeth is one who holds still upon certain occasions . The distinction between voluntary and involuntary behavior is a matter of the kind of control . @ @ @ @ @ @ @ @ @ @ . The eliciting stimulus appears to be more coercive . Its causal connection with behavior is relatively simple and easily observed . This may explain why it was discovered first . The discriminative stimulus , on the other hand , shares its control with other variables , so that the inevitability of its effect can not be easily demonstrated . But when all relevant variables have been taken into account , it is not difficult to guarantee the result ? to force the discriminative operant as inexorably as the eliciting stimulus forces its response . If the manner in which this is done and the quantitative properties of the resulting relation warrant such a distinction , we may say that voluntary behavior is operant and involuntary behavior reflex . It is natural that the " will " as an inner explanation of behavior should have survived longer in the study of operant behavior , where the control exercised by the environment is more subtle and indirect . In the case of the operation we call reinforcement , for example , the current strength of behavior is due to events which have @ @ @ @ @ @ @ @ @ @ which are not observed at the moment their effect is felt . Deprivation is a relevant variable , but one which has a history of which we may have little or no information . When a discriminative stimulus has an effect upon the probability of a response , we see that the present environment is indeed relevant , but it is not easy to prove the inevitability of the control without an adequate account of the history of reinforcement and deprivation . Consider , for example , a hungry guest who hears his host say , // " Wo n't you come to dinner ? " ( We assume that the guest has undergone the elaborate conditioning responsible for the behavior described as " knowing English . " ) As the result of respondent conditioning , this verbal stimulus leads to a certain amount of " involuntary " secretion of saliva and other gastric juices and perhaps contraction of the smooth muscles in the walls of the stomach and intestines . It may also induce the guest to approach and sit down at the table , but this behavior is certainly @ @ @ @ @ @ @ @ @ @ determined , and we predict it less confidently . Both the salivary reflex and the operant response occur because they have usually been reinforced with food , but this history lies in the past , much of it in the remote past . In the absence of an appropriate state of deprivation they may not occur ; the guest may instead reply , " Thank you , Fin not hungry . " But even if the history of reinforcement and deprivation is satisfactory , the operant responses may be displaced by other behavior involving the same musculature . If our guest has been offended by undue delay in the preparation of the meal , for example , lie may take revenge by creating a further delay ? perhaps by asking to wash his hands and remaining out of the room a long time . The behavior has been acquired because it has been reinforced by its damaging effect upon other persons ? because the guest has " learned how to annoy people . " Before we can predict that he will come to the table as surely as we predict that @ @ @ @ @ @ @ @ @ @ relevant variables ? not only those which increase the probability of the response but also those which increase the probability of competing responses . Since we ordinarily lack anything like adequate knowledge of all these variables , it is simpler to assume that the behavior is determined by the guest 's will ? that he will come if he wants to and wills to do so . But the assumption is of neither theoretical nor practical value , for we still have to predict the behavior of the " will . " The inner explanation is no short cut to the information we need . If many variables are important , many variables must be studied . I'he distinction between voluntary and involuntary behavior , of operant and reflex behavior , parallels another distinction . Reflexes Pre primarily concerned , as we have seen , with the internal economy // of the organism , where glands and smooth muscles are most important . Reflexes employing the striped muscles are chiefly involved in maintaining posture and in other responses to the more stable properties of the surrounding world . This is the @ @ @ @ @ @ @ @ @ @ be acquired as part of the genetic equipment of the organism . Operant behavior , on the other hand , is largely concerned with that part of the environment in which the conditions for effective action are quite unstable and where a genetic or " instinctive " endowment is much less probable , if not actually impossible . Reflex behavior is extended through respondent conditioning and apparently can not be conditioned according to the operant pattern . Glands and smooth muscles do not naturally produce the kinds of consequences involved in operant reinforcement , and when we arrange such consequences experimentally , operant conditioning does not take place . We may reinforce a man with food whenever he " turns red , " but we can not in this way condition him to blush " voluntarily . " The behavior of blushing , like that of blanching or secreting tears , saliva , sweat , and so on can not be brought directly under the control of operant reinforcement . If some technique could be worked out to achieve this result , it would be possible to train a child to @ @ @ @ @ @ @ @ @ @ of his hands . A result which resembles the voluntary control of glands or smooth muscles is achieved when operant behavior creates appropriate stimuli . If it is not possible to alter the rate of the pulse directly through operant reinforcement , other behavior ? violent exercise , for example ? can generate a condition in which the pulse rate changes . If we reinforce a certain critical rate , we may in fact , though inadvertently , simply reinforce operant behavior which produces it . This effect appears to be the explanation of apparent exceptions to the rule . Cases have been reported in which a man could raise the hair on his arm " voluntarily . " Other subjects have been able to slow their pulse on command . But there is reasonable evidence for supposing that in every case an intervening step occurs and that the response of the gland or smooth muscle itself is not an operant . Other examples // in which an operant and a reflex are chained together in this way will be described in Chapter XV . It is not so easy to @ @ @ @ @ @ @ @ @ @ muscles through operant reinforcement . The difficulty is that an operant response may arise which simply imitates the reflex . One may sneeze , for example , not only because of the pepper but because of special social consequences ? " He only does it to annoy , because he knows it teases . " Whether the facsimile sneeze resembles the reflex response in every particular is hard to say , but it probably does not . In any case , the controlling variables are sufficiently different to warrant a distinction . The little boy who sneezes to annoy is unmasked when we set up conditions for incompatible operant behavior . If we offer him candy and the sneezing stops , we may be pretty sure that it was not reflex . We need not say that the sneezing must have been voluntary " because he could stop it when he wanted to . " A more acceptable translation reads , " IIe stopped sneezing when variables were introduced which strengthened competing behavior . " The distinction between voluntary and involuntary behavior is further complicated by the fact that the two @ @ @ @ @ @ @ @ @ @ system and the muscles of the eyelid take part in certain well-known reflexes . In the young child the reflex control sometimes acts alone , but operant behavior is later acquired which may become powerful enough to oppose reflex action . Ordinarily , breathing is reflex , but we " voluntarily " stop breathing under suitable conditions of operant reinforcelent ? for example , to win a bet or to escape the aversive stimulation f water in the nose when we dive . How long we stop will depend upon the strength of the breathing reflexes , which become more and more powerful as carbon dioxide accumulates in the blood . Eventually a point is reached at which we " can not help breathing . " the distinction between voluntary and involuntary behavior bears poll our changing concept of personal responsibility . We do not old people responsible for their reflexes ? for example , for coughing chrch . We hold them responsible for their operant behavior r example , for whispering in church or remaining in church while // coughing . But there are variables which are responsible for whispering @ @ @ @ @ @ @ @ @ @ just as inexorable . ' When we recognize this , we are likely to drop the notion of responsibility altogether and with it the doctrine of free will as an inner causal agent . This may make a great difference in our practices . The doctrine of personal responsibility is associated with certain techniques of controlling behavior ? techniques which generate " a sense of responsibility " or point out " an obligation to society . " These techniques are relatively ill-adapted to their purpose . Those who suffer are the first to speak out for the inevitability of their behavior . The alcoholic insists that he ca n't help drinking and the " victim of a bad temper " that he ca n't help kicking the cat or speaking his mind . We have every reason to agree . But we can improve our understanding of human behavior and greatly strengthen our control by designing alternative practices which recognize the importance of reinforcement as well as other variables of which behavior is a function . DISCRIMINATIVE REPERTOIRES We have seen that any unit of operant behavior is to a certain @ @ @ @ @ @ @ @ @ @ of an integral organism . Although it may be analyzed into parts for theoretical or practical purposes , we need to recognize its continuous nature in order to solve certain common problems . Discriminative behavior offers many examples . In the behavior of reaching toward and touching a spot in the visual field , each position which the spot may occupy requires a particular combination of reaching and touching movements . Each position becomes the distinguishing property of a discriminative stimulus which raises the probability of the appropriate response . Eventually a spot in any position evokes the movement which achieves contact with it . At the very edges of the field the behavior may be defective , and unusual cases may need special conditioning ? for example , reaching for an object seen in a mirror or from an unusual posture ? but in the central area all positions of the spot comprise a continuous field and all possible combinations of movements leading to contact form a corresponding field . The // behavior is acquired upon specific occasions when specific responses toward specific locations are reinforced , but the organism @ @ @ @ @ @ @ @ @ @ without referring to the punctate origins of the two fields . If we wish to specify the smallest possible unit of correspondence between stimulus and response , we use the dimensions in which the two fields are described . The correspondence is between points . But in many repertoires the minimal units fall considerably short of points in continuous fields . The stimuli and responses may not compose fields . When we learn the names of a large number of people , we do not expect either the visual patterns which the people present or their names to form continuous fields . The repertoire remains a collection of discrete units . Even when stimuli and responses can be described as fields , the behavior may not be developed to that point . In several of the discriminative repertoires now to be considered , the functional unit is much smaller than the stimulus or response which appears upon any given occasion and with which we characteristically deal , but it is by no means always so small as to be expressed as an instance of the correspondence between fields . Drawing from @ @ @ @ @ @ @ @ @ @ n which we live is so familiar that we are likely to forget how it is acquired . There are certain less familiar forms of behavior in which the origin of a discriminative repertoire can sometimes be clearly traced . In drawing " from copy " ? or , less obviously , from an object ? or behavior is the product of a set of three-term contingencies . A iven line in the material to be copied is the occasion upon which . rtain movements with pencil and paper produce a similar line . All arch lines and all such movements comprise fields , but the behavior may not reach a condition in which it can be dealt with as a field . This is easily seen in the behavior of the young child learning to draw . A small number of standardized responses are evoked by the highly complex stimulus field . The behavior of the skilled copyist is imposed of a much larger number of responses and may seem as ' natural " as our responses to spatial positions . It does not reach the iiiit at which it @ @ @ @ @ @ @ @ @ @ py is not reproduced exactly but rather with a characteristic // response in the " individual style " of the artist . An extreme case , in which behavior is divided into clearly identifiable discrete units even though the stimulus has the characteristics of a field , is the behavior of the electrical engineer who " draws a picture " of a radio set using perhaps twenty or thirty unit responses . There are great individual differences in the ability to draw from copy . The contingencies responsible for the behavior are by no means so universal as those responsible for spatial behavior with respect to the visual field , and very different amounts of instruction arc received by different individuals . Moreover , a small difference in early instruction may make a big difference in the eventual result . The child who develops at an early age a repertoire with which he successfully copies drawings and objects is likely to continue to use it and to receive further differential reinforcement . The special training of the artist includes many highly sensitive differential contingencies , supplied by a teacher or automatically @ @ @ @ @ @ @ @ @ @ " A man who can not draw well is likely to be puzzled by one who can . He can not see " how it is done . " By no " effort of will " can he produce a comparable achievement . The basic minimal repertoire is simply lacking . It can be established only through discriminative reinforcement . The behavior is under the control of the copy , not of the artist , and until the copy has been put into control through differential reinforcement based upon it as a discriminative stimulus , the behavior will not occur . Singing or playing by ear . Drawing from copy is like responding to the spatial world insofar as both stimuli and responses approach continuous fields in the same way in both cases . In playing an instrument or singing a tune " by ear , " however , spatial dimensions are lacking . Here appropriate repertoires are set up by similar three-term contingencies . A tone is the occasion upon which certain complex behavior in the vocal apparatus will be reinforced by generating a matching tone . The reinforcement is @ @ @ @ @ @ @ @ @ @ with respect to good matches , or supplied by someone ? an instructor , for example ? whose behavior also reflects goodness of match . Such a repertoire may also include responses to intervals , each heard interval being the // occasion upon which a complex response generating a corresponding interval is reinforced . Melodies , harmonic progressions , and so on , may form the bases for similar repertoires . The same kind of relationships may govern the playing of a musical instrument , where the topography of the behavior which generates the tones or patterns is entirely different . The ultimate unit in singing or playing by ear may stop at the level of the half-tone scale . Both stimuli and responses usually show this " grain . " A singer with poor pitch is one whose response system has a poorly defined grain which does not match the stimulus system . On the other hand , a singer with good pitch may correctly sing a melody which is itself defective . Here the response repertoire is in better focus than the stimulus . The half-tone scale is not @ @ @ @ @ @ @ @ @ @ vocal mimic has a repertoire which approaches a continuous field and which permits him to duplicate nonmusical sounds . The successful imitation of bird song or of the noise of machines requires this sort of fine-grained repertoire . We easily lose sight of the conditioning required to develop such behavior . The individual who can not mimic an auditory pattern or who can not sing or play by ear is likely to be puzzled by one who can . He finds it quite impossible to sing a matching pitch or to hum a corresponding tune or to imitate the noise of a locomotive , and he has no conception of how the successful mimic does so . He can not be a successful mimic by any " act of will . " The difference lies in the histories of reinforcement . If the repertoire with which one reproduces a melody has never been established , it will not be brought into lay by the appropriate circumstances . Imitation . It is only a short step from these discriminative repertoires to the field of imitation . So far as we know , @ @ @ @ @ @ @ @ @ @ mechanism . Such a mechanism would require that the stimulus generated by a given pattern of behavior in another organism elicit a series of responses having the same pattern ? for example , the visual stimulus of a running dog would elicit running in another dog . This would be an ex ' extremely complex mechanism and , in spite of a strong belief to the ontrary , it seems not to exist . Imitation develops in the history of // the individual as the result of discriminative reinforcements showing our same three-term contingency . The visual stimulation of someone waving a hand is the occasion upon which waving a hand will probably receive reinforcement . The auditory stimulus " Da-Da " is the occasion upon which the complicated verbal response which produces a matching auditory pattern is reinforced by the delighted parent . We see this sort of conditioning taking place in everyday life , and we can also set it up in the laboratory . For example , we can condition a pigeon to execute any one of several acts according to whether another pigeon is executing that particular @ @ @ @ @ @ @ @ @ @ a certain position , the imitator pecks a corresponding key . When the imitatee pecks a key in a different position , the imitator behaves accordingly . When the imitatee moves to the opposite side of the cage , the imitator follows . Such imitative behavior occurs only when specific discriminative reinforcement has taken place . Pigeons do not appear to imitate each other " naturally . " The necessary three-term contingency often occurs in nature , however . Thus , if a pigeon is scratching in a leaf-strewn field , this is an occasion upon which another pigeon is likely to be reinforced for similar behavior . The human parallel is not far distant . When we see people looking into a shop window , we are likely to look , too ? not because there is an instinct of imitation , but because windows into which other people are looking are likely to reinforce such behavior . So well developed is the imitative repertoire of the average person that its origins are forgotten , and it is easily accepted as an inherent part of his behavior . Imitative repertoires @ @ @ @ @ @ @ @ @ @ In learning to dance , a set of more or less stereotyped responses is acquired by virtue of which a step executed by the instructor is duplicated by the pupil . The good dancer possesses a large imitative repertoire of dance steps . When this repertoire is faulty , the imitation is poor , and the novice finds it very difficult to match a complicated step . In dancing , as in singing by ear , the imitative ability of a good performer seems almost magical to the untutored . A good actor possesses an imitative repertoire of attitudes , postures , // and facial expressions which enable him to follow the suggestions of a director or to duplicate behavior observed in everyday life . The attempts of the unskilled actor may be ludicrously wide of the mark because the essential repertoire is lacking . Although imitative responses approach a continuous field , that condition is probably never reached . The duplication of the stimulus is often not precise , and the " grain " of the repertoire with which even the good mimic duplicates behavior may be apparent . The @ @ @ @ @ @ @ @ @ @ function . We could easily establish behavior in which the " imitator " does exactly the opposite of the " imitatee . " Our second pigeon could be conditioned to peck always in a different position . Something of this sort is involved in ballroom dancing where the behavior of ' instructor and pupil in an " imitative " repertoire are not the same . In ballroom dancing a step backward on the part of the instructor is the occasion for a step forward on the part of the pupil . This kind of inverse imitation can become as smooth as behavior having the same properties , as the good " follower " shows . Other noncorresponding repertoires arc found in the field of sport . The behavior of the tennis player is controlled in large measure by the behavior of his opponent , but the corresponding patterns are not imitative in the usual sense . There is , nevertheless , a three-term contingency : subtle stimuli from the behavior of the opponent which are correlated with a forthcoming placement of the ball are the occasion for appropriate defensive behavior . @ @ @ @ @ @ @ @ @ @ of stimulation , and it is only because of this that he is able to get into proper defensive positions . Fencing offers an especially good example of the integrated behavior of two individuals in which a response on the part of one constitutes a discriminative stimulus for a different response on the part of the other . The behavior may be as closely integrated as that of two dancers executing the same steps at the same time . These inverse " imitative " repertoires can not approach continuous fields from which new instances will automatically emerge . To some extent , skilled dancers may improvise a dance in which one introduces a series of steps and the other follows , just as a tennis player is // to some extent automatically in possession of the proper reply to a new offensive maneuver , but the corresponding fields which provide for the duplication of behavior in true imitation are lacking . ATTENTION The control exerted by a discriminative stimulus is traditionally dealt with under the heading of attention . This concept reverses the direction of action by suggesting , not that @ @ @ @ @ @ @ @ @ @ that the observer attends to the stimulus and thereby controls it . Nevertheless , we sometimes recognize that the object " catches or holds the attention " of the observer . What we usually mean in such a case is that the observer continues to look at the object . An animated billboard is dangerous , for example , if it holds the attention of a motorist too long . The behavior of the motorist in attending to the sign is simply the behavior of looking at it rather than at the road ahead of him . The behavior involves conditioning , and , in particular , the special conditioning of the discriminative operant . The variables are not always obvious , but they can usually he detected . The fact that people read billboards instead of looking at the surrounding countryside shows how effectively reading is usually reinforced ? not only by billboards , but by stories , novels , letters , and so on . Powerful reinforcements are arranged by thousands of writers in every field of the written or printed word . All of these stimuli have common @ @ @ @ @ @ @ @ @ @ . Some reinforcement may also occur on the spot if the particular material is " interesting . " ( We saw in Chapter VI that " taking an interest " is only another way of expressing the consequences of operant reinforcement . ) We may study this relation in a simple experiment . We arrange to reinforce a pigeon when it pecks a key but only when a small light above the key is flickering . The pigeon forms a discrimination in which it responds to the key when the light flickers and not otherwise . We also note that the pigeon begins to watch the light . We might say that it is attending to it or that it holds its attention . The behavior is easily explained in terms of conditioned reinforcement . // booking toward the light is occasionally reinforced by seeing the light flicker . The behavior is comparable to looking for an object ( Chapter V ) . A steady orientation of the eyes is not the only possible result . The behavior of a lookout in the dark or in a heavy fog is an @ @ @ @ @ @ @ @ @ @ . The behavior of searching the field ? or responding to every part of it in some exploratory pattern ? is the behavior which is most often reinforced by the discovery of important objects ; hence it becomes strong . We can usually observe that the behavior with which a child looks for a misplaced toy is specifically conditioned . If some patterns of looking are reinforced by the discovery of objects more often than others . they emerge as standard behavior . We may study this in the pigeon experiment by arranging a series of lights , any one of which may begin to flicker as a discriminative stimulus . The pigeon comes to look at all the spots in a more or less random order . It may be said to be " looking for the flickering spot , " as in the example discussed in Chapter V. If the light begins to flicker while the pigeon is looking elsewhere , the flicker is seen at one side of the visual field . The behavior of looking directly toward the light is then optimally reinforced . We say that @ @ @ @ @ @ @ @ @ @ bird . But attention is more than looking at something or looking at a class of things in succession . As everyone knows , we may look at the center of a page while " attending to " details at the edges . Attempts to account for this in terms of " incipient eve movements " have failed ; and in any case no comparable orientation appears to occur in attending to features of an auditory pattern . Thus , when we listen to a phonograph recording of a symphony while attending particularly to the clarinets , it is apparently not possible to demonstrate any special orientation of the ear . But if attention is not a form of behavior , it does not follow that it is , therefore , outside the field of behavior . Attention is a controlling relation ? the relation between a response and a discriminative stimulus . When someone is paying attention he is under special control of a stimulus . We detect the relation most readily when receptors are conspicuously // oriented , but this is not essential . An organism is attending to @ @ @ @ @ @ @ @ @ @ receptors are oriented to produce the most clear-cut reception , if its behavior is predominantly under the control of that detail . When our subject describes an object at the edge of the page even though we are sure he is not looking at it , or when he tells us that the clarinets have fallen a beat behind the violins , we need not demonstrate any spatial arrangement of stimulus and response . It is enough to point to the special controlling relation which makes such a response possible . Similarly , in our experiment , we can say that the pigeon is attending to the light , even though it is not looking at it , if it consistently makes the correct discriminative reaction ? if it strikes the key when the light is flickering and does not strike it when the light is still . It will probably look at the light because the contingency responsible for the " attention " is also responsible for the reinforcement of such behavior , but it need not do so . When we enjoin someone to pay particular attention to a @ @ @ @ @ @ @ @ @ @ discriminative stimulus which supplements the stimulus mentioned in controlling the behavior of the observer . The observer is conditioned to look at or listen to a particular stimulus when he is told to " pay attention " to it because under such conditions he is reinforced for doing so . People generally say " watch that man " only when that man is up to something interesting . They generally say " Listen to the conversation in the seat in hack of you " only when something interesting is being said . Just as we may attend to an object without looking at it , so we may look at an object without attending to it . We need not conclude that we must then be looking with an inferior sort of behavior in which the eyes are not correctly used . The criterion is whether the stimulus is exerting any effect upon our behavior . When we stare at someone without noticing him , listen to a speech without attending to what is said , or read a page " absent-mindedly , " we arc simply failing to engage in @ @ @ @ @ @ @ @ @ @ of such stimuli . // I The environment is so constructed that certain things tend to happen together . The organism is so constructed that its behavior ranges when it comes into contact with such an environment . There c three principal cases . ( 1 ) Certain events ? like the color and taste ripe fruit ? tend to occur together . Respondent conditioning is the responding effect upon behavior. ( 2 ) Certain activities of the organism effect certain changes in the environment . Operant conditioning is the corresponding effect upon behavior. ( 3 ) Certain events the occasions upon which certain actions effect certain changes the environment . Operant discrimination is the corresponding ea upon behavior . As a result of these processes , the organism hick finds itself in a novel environment eventually comes to behave an efficient way . The result could not be achieved by inherited mechanisms because the environment it not sufficiently constant from 1c generation to another . It is also characteristic of the normal environment that events occur together in certain temporal relations . A stimulus may precede l other stimulus by @ @ @ @ @ @ @ @ @ @ A response may produce a consequence only after a given interval , as when the ingestion of alcohol is followed by typical effects after a certain delay . A response may achieve its consequence hen executed at a given time after the appearance of a discriminavc stimulus , as when a ball can be hit only by swinging at it after has come within reach and before it goes out of reach . The first two of these characteristics raise no special problem . The effect of an interval of time between the stimuli in respondent conditioning is easily stated . If we give food to an organism ten seconds ( ter a neutral stimulus has been presented , the process of conditioning ; follows essentially the usual pattern : the dog salivates to the acv iously neutral stimulus . But eventually a temporal discrimination established . The dog does not salivate when the conditioned stimulous is first presented but only after an interval has elapsed which gradually approaches the interval after which the unconditioned // stimulus usually appears . We may deal with this result simply by defining the conditioned @ @ @ @ @ @ @ @ @ @ many units of time . The introduction of an interval of time between response and reinforcer in operant conditioning is also of little interest here . The effectiveness of the reinforcement is reduced , but the behavior is not otherwise greatly changed . When temporal properties are added to the three-term contingency of the discriminative operant , however , special effects follow . A response is sometimes reinforced only if it is made as rapidly as possible after the appearance of a given stimulus . A contingency of this sort is responsible for the speed with which many people rush to answer the telephone . Picking up the telephone and saying " Hello " is reinforced only if the response is made quickly . The runner responds to the starting gun in the same way for the same reason . In a typical " reaction-time " experiment a subject is instructed to lift a finger off a key as soon as a light has appeared or a tone has sounded , with the result that the behavior comes to occur " as soon as possible . " Although the instructions given @ @ @ @ @ @ @ @ @ @ starting a race are complex , the effect upon behavior is due to the simple three-term contingency with an added temporal specification . This same contingency will cause a pigeon to behave " as quickly as possible " also . The pigeon 's reaction time is of approximately the same magnitude as man 's . A response may also be reinforced only if it is delayed by a given interval of time after presentation of the stimulus . Thus , a pigeon is reinforced for pecking a key only if it waits , say , six seconds after the key is presented . Many social and commercial reinforcements are of this sort ? where , for example , the net effect is reduced if one replies too quickly or agrees too readily to an arrangement or where an optimal reinforcement follows only after " due deliberation . " Under contingencies of this sort , the maximal probability of response is characteristically reached a little before the required interval has elapsed . A characteristic effect of a delay is sometimes referred to as " expectancy " or " anticipation . " Let @ @ @ @ @ @ @ @ @ @ to give a child a hit of candy a few minutes after // arrival . How can we formulate the behavior of the child in " anticipatinng " the gift of candy ? We may note , first of all , that the arrival the visitor serves as a conditioned stimulus and that the child 's mouth will possibly water . If the interval which elapses between the iva and the presentation of candy is fairly uniform , a temporal crimination may develop so that this conditioned response will t appear until the interval has almost elapsed . If certain movecuts on the part of the visitor have usually preceded the presentan of candy , any movement on the part of the visitor will be reinwing . The child will therefore " attend " to the visitor , as that rill has just been defined . He will watch the visitor closely . If any bal stimuli have been especially correlated with the candy , he ' II also listen to whatever the visitor is saying , since listening will ve been reinforced by such stimuli . Any behavior on the @ @ @ @ @ @ @ @ @ @ candy more probable s also been reinforced and will be strong . The child may make ttiself conspicuous by " showing off , " for example . For the same son he may refer to former gifts and thus supply a " hint " to the itor ( Chapter XV ) . Much of the child 's behavior will be emotional . It is easier to serve this when the " anticipated " stimulus is aversive . As we shall in Chapter XI , the emotional state in such a case is called nxicty . " When the anticipated stimulus is positively reinforcing , ere is a general change in the behavior of the child in the direction greater excitement and responsiveness . These are to some extent e strengthening aspects of " joy " or " delight . " ( We shall see in haptcr X that these terms must be used with caution . ) ' I'licre is still another ingredient in " anticipation . " The behavior the runner in response to the words " Get ready , get set . . . " ows all the effects @ @ @ @ @ @ @ @ @ @ Ise , respiration , sweating , and so on , ( 2 ) a special controlling relatn to the voice of the starter which we call " close attention , " and 3 ) emotional changes which , if the race is to be a grueling one , will more characteristic of anxiety than of joy . In addition to all this , c runner tenses his muscles and adopts just that posture which ill make his response to the signal " Go ! " most effective . This kind // of behavior , sometimes called " preparatory set , " is reinforced by the increased speed of the response which follows . The behavior may be nothing more than a partial execution of the response of " going , " sometimes revealed in the false start , or it may consist of any other form of behavior which receives the net reinforcement of a more successful start ? for example , holding still rather than rocking back and forth on one 's toes . CHAPTER VIII THE CONTROLLING ENVIRONMENT THE IMPORTANCE OF THE ENVIRONMENT No matter what our philosophy of @ @ @ @ @ @ @ @ @ @ that the world about us is important . We may disagree as to the nature or extent of the control which it holds over ns , but some control is obvious . Behavior must be appropriate to the occasion . Failure to keep in touch with reality leads to the kinds of difficulties often observed in psychotic behavior . Even when a man is engaged in rejecting the world , in systematically reducing certain forms of its control over him , he is physically interacting with it . Many theories of human behavior , nevertheless , neglect or ignore the action of the environment . The contact between the organism and the surrounding world is wholly disregarded or at best casually described . This is almost always true in clinical psychology , for example . The clinician often speaks of people , places , and things as " facts " entering into the interpretation of his patient 's behavior , without further specifying their action . This practice may be adequate for certain purposes of communication , but it must be expected to fail at some point . Some of the @ @ @ @ @ @ @ @ @ @ reached . A case history may inform us , 129 // for example , that on a given day the patient saw an acquaintance who was approaching him cross the street , and this event may be regarded as significant in interpreting the patient 's behavior . But the report " X saw Y cross the street " does not prepare us for many possibly relevant questions . For example , what are the important properties of visual patterns which lead X to say , " That is Y " ? ' Was X 's report of this event determined by a clear visual stimulus possessing these properties ? in other words , was it really Y or did X merely " think it was Y " ? In the latter case how plausible was the mistake ? How much of the effect upon X was due to the appearance of Y as a person and how much to Y 's behavior in crossing the street ? Upon what past occasions had similar stimuli affected X , and what conditioning had taken place with respect to people who cross streets , @ @ @ @ @ @ @ @ @ @ was X 's reaction due to a condition which we may describe by saying that X was " afraid that Y was avoiding him " ? Did earlier conditioning with respect to people who cross streets really involve Z , who resembles Y , and if so , may we say that Y was serving as a " symbol " for Z ? Questions of this sort are frequently treated in the later discussion of a case history , but often they would not arise if the earlier analysis of the contact between organism and environment had been adequate . An improved analysis would mean , not necessarily more information in any particular instance , but rather an understanding of the ways in which stimuli generally work . The casual account ignores many important points . THE ANALYSIS OF STIMULI In studying the extremely important independent variables which lie in the immediate environment , we may begin with a physical description . What is the structure of the world which we see , hear , touch , smell , and taste ? We should not prejudge these events from their effects @ @ @ @ @ @ @ @ @ @ the usual terms of the physics of light and sound , the chemistry of odorous or tasteful substances , and so on . We are interested , of course , only in conditions or events which have an effect upon behavior . The electromagnetic radiation of radio and television has // no effect upon the unequipped organism , except perhaps at very high energy levels . We do not say that the radiation is " not a stimulus because it does not stimulate . " We simply ignore it just as we ignore the color of the apparatus we use in the study of mechanics as soon as we find it to be irrelevant . The kinds of events which stimulate the organism are effective only within certain limits . We hear sound , but only of certain pitches and intensities . We see light , but only of certain intensities and wave lengths . The limits of stimulation , and also the smallest differences in stimuli which make detectable differences in behavior , have been extensively investigated . The normal individual differs from the blind or color-blind in his reaction @ @ @ @ @ @ @ @ @ @ in his reaction to tones , from the anosmic in his reaction to odors , and so on . Smaller differences between normal individuals may be equally important . Research of this sort often emphasizes the action of the organ where the interchange with the environment takes place ? the eye , the ear , the taste buds in the tongue , and so on ? but the whole organism may be involved . What appear to be simple sensory reactions often depend upon variables in the fields of conditioning , motivation , and emotion . Several important problems concerning stimulation are relatively independent of the particular physical properties of stimuli and of their range of effectiveness . In attacking these problems it does not matter whether the receiving organ is the eye or ear , for example , and we may work with values of stimuli which do not raise the problem of limits . In discussing the stimulus functions of elicitation , discrimination , and reinforcement , it was not always necessary to specify the nature of the stimulation , and we shall see in Chapter IX that this @ @ @ @ @ @ @ @ @ @ . field of emotion . There are even more general processes which may be studied not only without considering the particular form of energy exchange at the periphery of the organism , but also without specifying whether the stimuli are eliciting , discriminative , reinforcing , or emotional . In the following discussion the discriminative stimulus will be emphasized , but each process could presumably be demonstrated in the other functions as well .                     