@@746677

A college-based teacher preparation program undertook a grant-supported evaluation of its ' curriculum focused on preparing teacher candidates for integrating technology into instruction . The project employed a pre posttest design , including the use of the 54-item Educator 's Knowledge and Implementation of Technology instrument ( EKIT ) , which provides information regarding technology-related capabilities summarized in five area subscores and a total score . Results of the study substantiated the usefulness of such instrumentation in a pre-posttest design for evaluating program impact on students , and for prioritizing areas for continuous program improvement based on low achievement and unsustained growth . Conclusions clearly establish the power of the pre-posttest design for the evaluation and continuous improvement of teacher training programs . <p> Educators struggle with two demands that cause them to lose sleep ( Blasik , 2002a , 2002b ; Lewis 2002 ; Shaha , 2002 ) . First is the need to prove that their programs are effective by validating them based on results , and second is the need for continuous program improvement . Sources of educational funding come with the program validation requirements @ @ @ @ @ @ @ @ @ @ increasing accountability , program funding and continuity are accompanied by expectations of proof that offerings are beneficial for student learning and meet objectives and requirements . <p> The second demand centers on the need to continuously improve instruction and the impacts it achieves . Continuous improvement reflects a clear focus on identifying where things need to improve , followed by a systematic approach to implementing program changes designed to remediate prioritized weaknesses and thereafter measure impact ( Arcaro , 1995 ; Brown , 2001 ; Boulmetis , 2000 ; Ross , 1993 ) . Ongoing program improvement is integral to any project with an evaluation component or that seeks to achieve lasting success ( Quiones &; Kirshstein , 1998 ; Smith , 2002 ) . The overall objective is to achieve ever-higher levels in tangible measures of educational success . <p> Assessment , well designed and executed , helps educators resolve the demands for validation and continuous improvement ( c.f. Baldrige , 2002 ; Brown , 2001 ; Walton , 1986 ) . Through appropriate assessment approaches , educators can make goals and objectives tangible and evaluate whether they have been @ @ @ @ @ @ @ @ @ @ Shaha , 1997 ; Smith , 2002 ) . Educators can then identify areas of success and strength , and isolate and prioritize areas for improvement . Clearly validation and improvement are best achieved when outcomes and desired results are clearly identified , translated into plans , and then convened into instruments designed to gather the requisite information regarding success ( Arcaro , 1995 ; Baldrige , 2002 ; Quiones , 1998 ; Stevens , 2001 ) . To show gains in performance and program improvement , organizations must measure impacts and outcomes , and critically examine the results to achieve excellence ( Blasik , 2002a , 2002b ; Daniels , 2002 ; Shaha , 1997 ) . <p> Increases in accountability and the demand for continuous improvement have also affected programs focused on preparing teachers to better use technology and incorporate it into instruction . In 1999 , Utah Valley State College ( UVSC ) received grant funding from the U.S. Department of Education through Preparing Tomorrow 's Teachers to Use Technology ( PT3 ) . The application required a substantial evaluation component . UVSC 's evaluation plan included a @ @ @ @ @ @ @ @ @ @ measure the impacts of technology instruction on teacher candidates . Findings from the resulting assessment tools were used to determine what was learned and retained , and then to identify and implement program improvements focused on enhancing learning outcomes . The results of the assessments were designed and leveraged to improve the teacher education program ( Farnsworth , 2002 ) . <p> The evaluation plan for the PT3 capacity building grant purposely embedded the fundamentals of continuous improvement within a well conceptualized measurement approach . The objective was to understand and improve the technology focused instruction provided to teacher candidates ( Farnsworth , 20021 ) . Information gained from the assessment tools used led to a better understanding of the effects of teaching technology to teacher candidates and to achieving and sustaining college based program improvements . Execution of the evaluation plan provided program designers at UVSC with data that led them to raise the capabilities of their program and student beneficiaries to the next level . The study reported illustrates the impact of an approach where " assessment was for learning not only of learning " ( Stiggens , 2002 @ @ @ @ @ @ @ @ @ @ designed to build the process for using assessments and subsequent analysis for program improvement in two areas : 1 ) determining the technology knowledge and skills of teacher candidates ; and 2 ) determining the performance level of these skills and their persistence over time as a result of college-based classroom instruction . Teacher candidates took the Educator 's Knowledge and Implementation of Technology ( EKIT ) as a means for assessing their sell ' perception of technology-based skills and knowledge of implementing technology in instructional design and curriculum ( see Appendix A ) . The EKIT assessment tool was adapted from the survey tool designed by the Utah Educator 's Network ( UEN ) . <p> The EKIT was also critical for validating the efficacy of teacher preparation curriculum in enhancing the technology skill levels of teacher candidates . Scores on the EKIT provide insights into specific 54 item based areas of competency and achievement , as well as cumulative competency as subscores in particular areas of technology skills and utilization . The items within the assessment tool are grouped into subscores as follows : <p> A. Instructional Design : @ @ @ @ @ @ @ @ @ @ <p> B. Supporting Instruction : Items 31-35 , with the cumulative subscore labeled SubSupt . <p> C. Internet and Instruction : Items 36-45 , with the cumulative subscore labeled SubIntnt . <p> D. Classroom Management : Items 46-49 , with the cumulative subscore labeled SubClsrm . <p> E. Basic Teacher Computer Skills : Items 50-54 , with the cumulative subscore labeled SubSkill . <p> Two cohorts of undergraduate education students , teacher candidates , completed the EKIT at the start and end of the 20002001 academic year . <p>  --  The Junior cohort ( n=62 ) completed the EKIT prior to their coursework on technology skills and incorporating technology in classroom instruction for elementary students , and again at the end of the academic year and the close of that instruction . <p>  --  The Senior cohort ( n=51 ) received the same coursework on technology skills and incorporating technology during their corresponding junior year . They completed the EKIT at the start and end of their fourth year in the education preparation program , during which time no additional instruction on technology integration took place . Results <p> Figures 1 @ @ @ @ @ @ @ @ @ @ the pre and posttest occasions . There are three areas of importance within the graphs : The Total or cumulative scores are shown as the final , or right-most dots on the graphs . The subscores for each major portion or content area are shown as the last five dots prior to the Total . Individual item scores for each group are shown within the remainder of the graph , following the sequence within the instrument . <p> Figure 1 illustrates pre-test ( i.e. pre-instructional ) results for juniors , and results at the start of the academic year for seniors that experienced the instruction during the previous year . The seniors , having already experienced the instruction , were significantly better prepared than their entering junior counterparts on the pre test ( F=212.52 , df=1 , 100 , p=.001 ) . While this seems intuitively obvious , it is only because the same assessment instrument was applied at the start and end - a pre-post design  --  of each academic year that such conclusions can be drawn . <p> Figure 2 illustrates the posttest , or post-instruction scores for juniors @ @ @ @ @ @ @ @ @ @ not experience any additional instruction during that academic year . <p> Program Impact . A first obvious conclusion from the graphs is that the program 's impact on juniors was significant as evidenced by contrasting their pre ( Figure 1 ) and post ( Figure 2 ) total scores ( F=141.82 , df=1 , 120 , p=.001 ) . The benefits of pre and post assessment are further illustrated by examining the change in assessment results among seniors , wherein seniors suffered a statistically significant loss in total scores between the pre ( Figure 1 ) and posttest ( Figure 2 ) occasions ( F=10.06 , df=1,89 , p=.002 ) . <p> Program Weaknesses . The Total posttest scores for juniors and seniors are not significantly different ( see Figure 2 ) . However there are some interesting differences between the groups on subtest scores . For example , the post subtest scores for Classroom Management Software ( SubClsrm ) were significantly lower for juniors than for seniors ( F=24.89 , df=1 , 108 , p=.001 ) . This can be further understood by glancing to the left on Figure 2 @ @ @ @ @ @ @ @ @ @ items related to classroom management software . Upon further examination of the curriculum offered to the juniors , it was validated that instruction on classroom management software was diminished from what was offered to the seniors during the previous year . <p> Often , examination of cumulative subscores alone can mask important detail . For example , subtest scores for Basic Teacher Computer Skills were not significantly different between juniors and seniors on the posttest as shown in Figure 2 . However , further examination revealed that there are significantly lower scores on two items related to instruction that juniors received in two important areas , including spreadsheet programs ( F=13.38 , df=1 , 109 , p=.001 ) and database software ( F=6.42 , df=1 , 108 , p=.013 ) . Curriculum for seniors included these two factors in the previous year , but juniors did not benefit from that same instruction . <p> Sustaining Program Impact . Analysis of the subtest scores led to another set of conclusions . Juniors and seniors finished on equal ground for some subtest scores ( see Figure 2 ) . For example , the @ @ @ @ @ @ @ @ @ @ Internet and Instruction ( SubIntnt ) showed no significant difference between juniors or seniors . This indicates that instruction the juniors received put them on equal ground with seniors who received instruction the prior year , and that gains seniors experienced from the prior year were sustained . <p> In contrast subtest scores for Instructional Design ( SubDesgn ) indicated that seniors lost capabilities between years ( F=19.26 , df=1 , 89 , p=.001 ) . Additionally , nearly the entire range of items within the Instructional Design subscore in Figure 2 showed juniors scoring higher than their senior counterparts instructed the year before ( F=8.76 , df=1 , 109 , p=.004 ) . Therefore , not only did the juniors show better results for the Instructional Design , but also seniors declined from where they were at the beginning of that academic year in this particular area . Two questions in the Instructional Design , Items 7 ( TCLesn : I write technology-based lesson plans and curricula ) and 8 ( TCPlan : I plan technology-based lessons to teach technology skills ) are examples of specific areas in which seniors @ @ @ @ @ @ @ @ @ @ ; F=21.39 , df=1 , 89 , p=.001 ; respectively ) . Discussion and Conclusions <p> Several important conclusions can be drawn regarding the technology-focused college-based instruction provided to teacher candidates : <p>  --  The instruction benefited teacher candidates , as evidenced in pre versus posttest scores . <p>  --  Lack of reinforcing instruction during the senior year led to an erosion in some technology skills , while other skills were sustained without retrenchment . <p>  --  Analyses of subtest and item-level data yielded detailed guidance for specific areas of instructional improvement . <p> More important than conclusions about the program , however , are those that were gained regarding the assessment and validation approach employed . The depth and kind of program-level understanding illustrated in this study can not be achieved without adhering to the principles and processes embodied in the assessment approach implemented : the repeated , disciplined use of a single assessment tool fully aligned with the outcome objectives of the instructional program , all within a pre and posttest design . This approach was fundamental to assessing candidate learning , validating program success and prioritizing areas for @ @ @ @ @ @ @ @ @ @ the PT3 grant established the foundation for leveraging the principles of quality and continuous improvement . <p> The UVSC education department was able to recognize how well they met the goals and objectives they set for their curriculum . Then the department was able to isolate and prioritize areas for improvement and quantify the impact of improvements implemented . The findings of this study establish the value of assessment and evaluation based on the consistent use of objectives-focused tools within a pre-posttest design , for validating program efficacy and achieving continuous improvement . <p> Correspondence concerning this article should be addressed to Valerie K. Lewis , Director , Performance Assessment and Evaluation , Performance Learning Systems , Inc. 2174 New Horizon Drive , Sandy , Utah 84093 ; email : vlewis@plsweb.com <p> GRAPH : Figure 1 . Pre-test results measuring the junior and senior cohort . <p> GRAPH : Figure 2 . Posttest results . <p>                     