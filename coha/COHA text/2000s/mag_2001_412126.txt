@@412126

BITS ARE DYING . LONG LIVE LIGHT . What is a computer ? <p> In the early days , computers were primarily glorified ( but fast ) calculators . The very name " computer " was even borrowed from an existing profession : people whose job it was to compute numbers . In the first half of the 20th century , when someone said , " I will have these numbers checked by our computer , " he was referring to a human being . <p> But how often do you use your PC to compute numbers ? The functions of the computer these days seem mostly nonnumerical  --  word processing or surfing the Internet . But with all these applications , the information itself is stored as binary numbers . The computers still compute them ; only now they represent words , sounds and images : <p> Similarly , when you use digital telecommunication , your voice and data are fragmented into packets and dispersed over trunk lines and switching nodes . In data transfer from , say , Chicago to Atlanta , parts of your conversation @ @ @ @ @ @ @ @ @ @ switched through Baltimore . Which parts are sent where all depends on the most effective path in terms of cost and load . Furthermore , your bits are interleaved with those of other subscribers . Such complex switching can only be performed by high-end , high-speed computers that constantly calculate how to break apart the signal , send it through the best nodes and put it all together again at the destination . So we are already relying on computer " intelligence " simply to move information around on the Internet . <p> An intelligent network requires more than mere speed ; it also requires connectivity . There are currently at least 100 million host computers on the Internet . Each averages a million logic circuits  --  100 trillion tiny decision-makers , all connected through the Internet . That exceeds the number of neurons in a human brain by at least a factor of 10,000 . The collective brainpower of a small town ? <p> In the mid-1990s , researchers conceived the idea of using the vast idle computing power represented by just the PCs connected to the Net to perform @ @ @ @ @ @ @ @ @ @ search for extraterrestrial intelligence  --  the SETI project , for short . By creating a computer application analogous to a screen saver , the researchers were able to give each PC owner the ability to crunch the numbers on parts of the immense data set . Periodically , each computer would send its results automatically back to a central computer , where the results were compiled and assessed . At the peak of the project , 500,000 PCs were enlisted . <p> The SETI Project was a bold new approach to massively parallel , distributed processing . But there is a bottleneck that keeps the Net 's computers from being truly interconnected  --  the awkward , slow crossover between the electronic processors and the photonic , light-driven Internet . That gap is constantly shrinking ; when optical fibers reach directly into the box and connect with the optical data buses of hybrid opto-electronic computers , then the last significant barrier between the network and the computers will be gone . When that happens , the network will become a fast , fully interconnected neural network , with the potential for real @ @ @ @ @ @ @ @ @ @ all . TAKE THE TEST <p> Alan Turing , the brilliant British mathematician who helped break the secret Enigma code of the Germans in World War II , was one of the founding fathers of modern computer science . Turing was interested in finding out what tasks were broadly computable versus those that were not . His method was to hypothesize an idealized computing machine , unconstrained by considerations of cost or the time it might take to perform a given calculation . <p> Turing 's quest to define a " universal " computer was highly abstract , but it had far-reaching practical consequences . For though his " machine " was meant only to be a set of abstract functions , his conceptual model contained many of the components that later became standard in modern computers . For instance , a processing unit that makes decisions by comparing input data to the internal state of the unit ; the notion of a program of logical steps ; and the idea of a memory to store intermediate results of the computation  --  all were there in the original 1937 concept of @ @ @ @ @ @ @ @ @ @ to ascertain when a machine could be said to be intelligent . The test consists of an Examiner , who poses questions via a keyboard to both a human subject and to a potentially intelligent machine . The Examiner can ask any question of either of the two . Both subjects have the goal of convincing the Examiner that they are human . To achieve this end , the human and the machine can answer the questions truthfully or can lie if that best achieves their goal . If , at the end of a sufficiently long testing period , the Examiner can not distinguish which of the test subjects is the computer and which is the human , then the computer is deemed " intelligent . " <p> Turing 's test is a wonderfully pragmatic approach to the problem of intelligence . If the machine behaves in a manner that is indistinguishable from an intelligent human , who is to say that the machine is not intelligent ? <p> The optical fabric that will drape the world , with its multiplicity of nodes and interconnectivity , may indeed meet the @ @ @ @ @ @ @ @ @ @ of a human , then perhaps it will deserve the name of " intelligent being " that we guard so jealously for ourselves . It will become our constant ethereal companion as we live our lives in continuous connection to the optical network . In the distant future , we may even plug ourselves into the network through neuro-optical interfaces and become the intelligent nodes to replace the artificial ones . What , then , might we accomplish ? THE TELLING IMAGE <p> A crucial feature is missing in the quest for machine intelligence : the computer as we know it represents the world in digital bits  --  ones and zeros . Do bits have enough depth or nuance to implement optical intelligence in the time scale of human thought ? <p> How do we move beyond the serial intelligence of fiber to machines in which images become the units of information ? In these " visual " machines , images control images . Such computers  --  if that is the right word  --  are not necessarily digital in nature , nor are they necessarily " universal , " in the @ @ @ @ @ @ @ @ @ @ processing and analyzing visual information . <p> These machines represent the evolutionary end point of visual and optical communication . Where human visual capability and speed end , artificial visual machines begin and go beyond human experience . When 100 million pixels connect independently with 100 million others , the number of interconnections exceeds that in the human brain . In such a computer , images are both data and program . One image " tells " the computer what to do with another . This computer would be highly reconfigurable , because functions would be defined by the images sent into the computer . The architecture would remain wide open , like the brain of a newborn child , with potential to be molded into any number of possible configurations . <p> The parallelism we would gain by moving from a single dimension  --  bits per unit of time  --  to two dimensions  --  images per time  --  is tremendous . Wavelength-division multiplexing currently pushes physics to the limit just to get a thousand different wavelengths propagating down a fiber together . But an image the size of only one @ @ @ @ @ @ @ @ @ @ parallelism almost a million-fold . The immense advantage is clear . The challenge is to develop the architecture that can tap it . MAKING MONA LISA <p> One technology is ideally suited to exploiting massive interconnection : optical holography . Holography was conceived in 1947 by the Hungarian-born electrical engineer Dennis Gabor  --  he received a Nobel Prize for it , in 1971  --  as a way of getting additional information out of electron microscopes . Optical holography only became practical after the invention of the laser . And today holograms most often are seen as flashy 3-D images on credit cards or on the cover of National Geographic . <p> But the true value of holography lies in the physical structure of the hologram , which is created when two beams of mutually coherent light interfere inside a holographic medium  --  a crystal or other material that can record the interference pattern between two or more light beams . This physical mixing of light-borne information streams is the basis of intelligent optical computers , along with the massive quantity of data that can be stored in pure image form . @ @ @ @ @ @ @ @ @ @ doubt have approved of the subject for the first extensive demonstration of holographic optical memory . In 1994 , a research group at Stanford University under the direction of Lambertus ( Bert ) Hesselink , a professor in aeronautic engineering , stored a full-color digital version of the Mona Lisa inside a holographic memory crystal . The image was written into the crystal and then read out , virtually error-free . The two images of the Mona Lisa  --  before storage and after the read-out  --  are indistinguishable to the eye . <p> The argument for holographic data storage is fairly easy to make on theoretical grounds . On the back of an envelope , I can write down the simple equations that show how holographic data storage can store a terabit of data in a crystal that is only a single centimeter on a side , with data transfer rates of a gigabit per second and random access times less than 100 microseconds . These specifications are simple consequences of the size of the wavelength of light , of the parallelism inherent in the optical image format and of the @ @ @ @ @ @ @ @ @ @ <p> These performance characteristics far outstrip any hard drive today , and certainly any that were available at the time of Hesselink 's experiment . But it still fell far short of the full potential of holographic storage . The total information stored was only about a million bits  --  not the thousand-fold higher terabit level that is desired . The data transfer rate of the system was only about a megabit per second  --  not the thousand-fold higher gigabit-per-second speed that everyone wants . The storage density was about ten million bits per cubic centimeter  --  not the 100,000-fold higher capacity that is sought . <p> In defense of the Mona Lisa experiment , it is important to point out that the first silicon memory chips did not perform even that well . In the early days of silicon integrated circuits , 163 kilobytes was an almost inconceivably large number . Any technology has a learning curve  --  Moore 's law , for instance . Silicon chips started modestly , but have continued to get faster , with greater transistor densities , for over thirty years . <p> Given time @ @ @ @ @ @ @ @ @ @ Not every computer on every desktop necessarily will one day have holographic memory . But it will surely find its way into the marketplace so much information is already in the optical domain that there will be a benefit to keeping it there . No matter how far engineers push silicon or magnetic storage , it will never be inherently compatible with optical information , especially optical information in image format . The bottleneck of conversion from optical to electronic and back will always be there . THROUGH A LOOKING GLASS <p> Holography has more to offer than 3-D storage of vast amounts of data . There is a logic to holograms , and with practice it is possible to gain an intuitive feel for how they behave . Reconstruction from holographic memory starts when we shine a reference beam onto the hologram : the hologram completes the picture by recreating the missing object beam . This is the general behavior of holograms : they complete the picture . Whatever beams were present when the hologram was recorded are recreated when the hologram is illuminated by just one of the original @ @ @ @ @ @ @ @ @ @ the process : supply the object beam but not the reference beam , and let the hologram complete the picture . The process of creating a " clean " beam has important uses in real optical systems . When a laser beam bounces off less-than-ideal surfaces , it produces optically scrambled beams that are hard to manipulate and detect cleanly . By passing such a " dirty " beam through a hologram , the information is " cleaned up "  --  turned into a uniform beam that is easy to send through lenses and into photo-detectors . <p> Let 's look at another example . Take a blank reference beam and run it backwards through a hologram . What will the hologram produce ? Think about making a video of a simple propagating wave . If you run the video backwards , by reversing the direction of the videotape and hence reversing the apparent direction of time , the wave propagates backward . Therefore , a plane wave ( or blank beam ) that travels backward ( by reflecting exactly back along its original path ) is equivalent to a time-reversed @ @ @ @ @ @ @ @ @ @ backward on a hologram , we get an object beam that behaves as if it were time-reversed and it propagates back until it converges on the exact surface of the original object . This makes it possible to create a time-reversal mirror . <p> A useful property of time-reversal mirrors is their ability to act as perfect retroflectors . No matter what direction the incident beam comes from , the reflected beam exactly retraces its path . A second , related property is its ability to send light rays back through distorting media . Consider smearing petroleum jelly on window glass and then trying to look through it . The jelly mixes up the rays and hopelessly distorts the image . But the time-reversal mirror makes it possible to see through such a distorting screen . The trick is to have a two-way imaging system . The object of interest lies behind the smeared screen and in front of a time-reversal mirror . The light is transmitted through the screen , through the object , then off the time-reversal mirror . The mirror makes the beam propagate back , retracing all @ @ @ @ @ @ @ @ @ @ beam emerges from the screen again , all distortion is removed  --  it runs exactly backwards to compensate for all the distortion it acquired going forward . What remains on the beams is the information of the object . <p> Yet another useful application of holograms borrows from the behavior of neural networks . In the previous cases of beam cleanup and time reversal , the reference beam carried no information . It was just a blank beam . However , both the object and the reference beams can carry different information ( different images ) . In this case again , the hologram completes the picture . First , the hologram is recorded when the film is exposed simultaneously to the two different object beams  --  say , a sphere and the edge of a box . Although both beams are now object beams , each acts as the reference for the other . The hologram then reproduces the missing beam . If we send in the sphere beam , it reconstructs the box beam . And if we send in the box beam , it reconstructs the sphere beam @ @ @ @ @ @ @ @ @ @ memory . The hologram " associates " the box with the sphere . When the hologram sees " box " it remembers " sphere " and vice versa . <p> In a neural net , the association is contained in the weights of the synaptic connections among the neurons . These are adjusted so that when one pattern is presented to the input neurons , the output neurons fire in such a way that they form the associated pattern . The distribution of weights constitutes the memory of the system . Present one pattern at the input and the network regenerates the target pattern at the output . <p> Notice how the language describing associative neural networks closely parallels the description of holographic recording and reconstruction . The recording period of the hologram is like the training period of a neural network . Throughout the hologram , tiny regions of periodic patterns of alternating optical density  --  gratings is the technical term  --  connect the features of one pattern with features of another . These numerous optical gratings are what scatter  --  or diffract  --  a light beam during the readout @ @ @ @ @ @ @ @ @ @ synaptic connections among input and output neurons ( the actual light beams ) . <p> Another parallel between holography and neural networks is the nature of the memory . In a neural network , memory is distributed . No single synapse records any specific feature of a pattern . Instead , all the features are distributed over all the synapses . This is why distributed memories remain robust in the face of damage or partial erasure . The memory might degrade a little , but all the features remain . The hologram works the same way . All features of an image are distributed throughout the volume of the hologram . Conversely , every small volume of the hologram contains information about the whole image . The smaller the hologram fragment , the less spatial resolution and clarity the reconstruction has ; but lower resolution is a small price to pay for the robustness of the memory . For many visual recognition processes , a coarse reconstruction is all that is needed for positive identification . <p> One last useful application of holograms makes it possible to extract specific features from @ @ @ @ @ @ @ @ @ @ containing text is sent through a lens , the lens breaks it apart into a combination of spatial frequencies  --  periodic patterns in the image . The letter E has a unique spectrum of spatial frequencies that differentiates it from all the other letters of the alphabet . By recording these rather than the image directly , it is possible to get a match between the spatial frequencies of one object with another test object . <p> The specific holographic process that does this is called content-addressable memory . That is , when we scan a text for the letter E , we are searching for specific content in memory . Traditional computers do this also , but if the content can not be ordered in a way that allows a binary search to be performed , then the search has to go sequentially through every element in memory . Thus , in a search for the letter E in a text , if there are N characters , the computer will step through all of them sequentially until the end of the list is reached . If N is a @ @ @ @ @ @ @ @ @ @ information on the Internet  --  the search might take an unacceptably long time , even with today 's high-speed computers . <p> Searching for content in an image , by contrast , is a parallel process . All the E 's are present at the same time and all share the same spatial frequency fingerprint . The search and identification of the locations take only as long as it takes light to pass through the optical components . Of course , reality intrudes . At present , the interface devices are opto-electronic  --  spatial light modulators and charge-coupled-device cameras connected to electronic computers  --  which limits their speed . This will change when computers begin to live more completely in the optical domain , with information stored in parallel data structures that can be accessed without needing electronic conversion . <p> Even with the opto-electronic interfaces , content searching of images has important applications . For instance , matching fingerprints is much more difficult than finding letters . Not only are all fingerprints different  --  representing an " alphabet " of three billion prints in the United States alone but every @ @ @ @ @ @ @ @ @ @ be different depending on who does the finger-printing or how the fingerprint is left behind by the individual . An optical " fingerprint computer " is a natural candidate to perform this task faster and better than any conventional computer . Identifying camouflaged enemy tanks in a cluttered battle scene , tracing retinal marks in a retinal security scan , finding faces in a crowd , locating Waldo in comic books  --  these are all tasks that require feature detection in an image . Where electronic computer programs struggle to do this task , optical computers do it naturally . THE HOLOGRAM IS THE COMPUTER <p> The key element missing in the standard hologram readout for associative memory is the decisiveness of the output of the optical neural network . If a degraded or incomplete object beam is presented , the recalled object will also be degraded or incomplete . If the degradation is severe enough , the reconstructed object is unrecognizable . But with human associative memory , there are many instances where human associative recall works in the presence of noisy or incomplete data . Something as simple as @ @ @ @ @ @ @ @ @ @ our natural neural networks to perform this task with little conscious effort . Likewise , visually , we rarely see a complete picture . Even when reading high-way signs or billboards , glare and haze or weak eyeglasses all conspire to scramble the visual message . Yet , we often do well in these situations , because our natural neural networks perform associative recall and classification on partial or noisy sensory data . <p> As humans , we are extraordinarily sensitive to the slightest nuances in facial symmetry and expression . This is because our visual neural networks have been honed since birth . Not only do we recognize faces that we see clearly ; we also recognize faces that are half obscured or partly in shadow or seen from odd angles , even partly from behind . <p> Not so for computers , the electronic kind anyway . To a computer , all faces look alike . We all have two eyes , a nose with two nostrils , two ears and a mouth . In the absence of hair or strong pigment , facial features are very similar among @ @ @ @ @ @ @ @ @ @ mug shot , images are taken under lighting conditions and angles that vary dramatically . Trying to get a computer to recognize a face under all this variability is considered to be one of the hardest tests for artificially intelligent visual recognition systems . <p> Associative holographic memory is one step toward generating a visual recognition system able to handle the nuances of face recognition . For the next step , it is necessary to add to the image correlator , an element that makes decisions . Decision-making requires non-linearity . Combined with feedback , it allows neural networks to eliminate wrong possibilities and provide clear identification . When this is done , not only does a perfect input elicit a perfect response , but any partial input will work as well , as long as enough information is present to allow the identification to be unambiguous . <p> An important feature that is missing in the analogy between holography and neural networks is non-linear response . In a neuron , non-linear response is important for thresholding  --  enabling one answer , and only one answer , to emerge from the @ @ @ @ @ @ @ @ @ @ time . In simple associative holography , the hologram responds linearly to the input ; imperfect inputs produce imperfect outputs . This is not how neural networks work . For a flexible neural network , even an imperfect input should produce a relatively good reconstruction , without crosstalk from other stored images . <p> The parallels between the massive interconnection properties of holograms and the interconnections of neural synapses in neural networks were first explored experimentally in the mid-1980s by a group led by Dmitri Psaltis , a physicist at CalTech in Pasadena , using light-emitting diodes and photodiode detector arrays . Their holographic neural network included an iterative feedback loop that incorporated gain to balance the losses incurred when passing around the loop . The purpose of the experiment was to take a partial or noisy image of a face and recall the complete face from the holographic memory . <p> Psaltis 's holographic optical neural network created a stir . It succeeded in demonstrating how concepts of neural networks could be combined with visual image-recognition using the full advantage of optical parallelism . As a proof of principle , @ @ @ @ @ @ @ @ @ @ hand , the non-linear thresholding device was slow and represented a bottleneck for the operation of the neural network . Furthermore , the fixed holographic memory required a separate and slow hologram recording process . <p> For these reasons , the overall performance of the system was not up to the level of electronic systems that did the same things at that time . The theoretical advantages of parallel optical processing in neural networks were outweighed by engineering problems . We are still waiting for this technology to gain an economic foothold with incentives for constant improvement . But in some ways , the door has already opened . The second , equally important function of any computer is processing . It is possible to have holograms that act as image-processing units , avoiding optoelectronic devices like the liquid crystal light valve . Crystals called dynamic holographic materials make this possible . MOVING PICTURES <p> Motion and change are key characteristics of the real world . To accommodate that , we need holography that can process light and images in real time . Recording and readout should occur at the same @ @ @ @ @ @ @ @ @ @ information on the beams changes , the hologram should change to match it . <p> Continuous real-time holography , called dynamic holography , is performed using non-linear optical crystals , with the recording beams and the readout beams all present at the same time . A dynamic hologram is generated by a transient change in the crystal 's optical properties in response to the periodic stripes of high and low intensity generated by the interfering beams . If the interference patterns move , the material responds accordingly . <p> The important improvement we gain by using a non-linear optical crystal to record holograms is the ability to compensate for a turbulent environment that vibrates and drifts ceaselessly . <p> The dynamic response of these holograms also benefits time-reversal mirrors . In real applications , laser beams are constantly being buffeted by a noisy environment . Furthermore , the time-reversed object beam might be the right beam for the object at the time when the hologram was recorded , but the object might have changed during the time of the recording and development of the hologram , making the static hologram obsolete @ @ @ @ @ @ @ @ @ @ a particular advantage when looking through turbulent media . For instance , telescopes have difficulty seeing through our atmosphere , either to image astronomical objects or to transmit laser communications to orbiting communications satellites . Atmospheric turbulence causes stars to twinkle at night and makes the sky blue and sunsets red . It 's also why the Hubble telescope had to be put in orbit , in order to get the most dramatic images ever of our universe . <p> Adaptive time-reversal mirrors fix these problems , and are already part of the growing field of adaptive optics , which covers a broad range of optical systems designed to conform to changing environments . Traditional adaptive optics uses sensors and actuators , connected by feedback electronics , to move mirrors or lenses , and compensate for time-varying changes in image quality . In sophisticated telescope systems , segmented " rubber mirrors " enable the actuators to move each section separately . <p> In contrast , holographic time-reversal mirrors require no active feedback or complicated software  --  they are " self-adaptive . " Appropriate changes in the hologram occur automatically as the @ @ @ @ @ @ @ @ @ @ this interference onto the hologram . Specialized time-reversal mirrors already have been commercialized as components in self-aligning laser cavities ; dynamic holographic gratings also play key roles in adaptive interferometers . DREAM MACHINES <p> At around the same time Psaltis 's group at CalTech was pursuing opto-electronic approaches to optical neural networks , an intensely imaginative assistant professor by the name of Dana Anderson had newly arrived at the University of Colorado . He was struck by the similarity between competitive learning in neural networks and the competition among laser modes inside a laser resonator . The competition causes one mode to win out and suppress the oscillations of the other modes . Because an optical resonator can support images , a mode can be an image . In this way , images can interact and compete directly . <p> By drawing on the massively parallel interconnection properties of holograms , Anderson made the analogy between competing laser modes and neurons more explicit . He constructed a ring resonator that used a holographic storage medium for the neural interconnections and a photo-refractive crystal called barium titanate . The resulting resonator was @ @ @ @ @ @ @ @ @ @ same dynamics , including mode competition . As one mode became more intense , its dynamic grating grew stronger , deflecting more of the pump beam into that mode and robbing energy from the others . The process iterates until only a single mode receives all the pump beam energy . Eventually only a single mode survives . <p> In the absence of any input , this resonator had a peculiar habit : " dreaming . " Slight drifts in the temperature of the resonator or intensity fluctuations caused by dust particles in the laser beams would constantly be trying to unseat the winning mode . Sometimes , these perturbations would be sufficient to usurp a winning mode , get the lion 's share of the pump beam energy and become the new king of the modes . <p> The output of such a resonator behaves like a person dreaming . If there are many different holograms stored in the holographic memory , the output of the resonator randomly recalls one , then another  --  like dreaming , drifting undirected through the day 's experiences . <p> However , when a @ @ @ @ @ @ @ @ @ @ mode that best matched the input quickly gained superiority over the other modes . If the injected information was ambiguous , matching more than one mode , then the system randomly switched among those modes , but not among the others that did not match . The recall therefore reflected the ambiguity of the input , while still eliminating the bad matches . <p> This work was important for three reasons . First , it equated the natural resonances of optical cavities with the natural responses of neural networks . Second , it moved optical neural networks into the all-optical regime , requiring no electronic interfaces . The dynamic holograms in the photorefractive crystal provided the decisionmaking non-linear threshold . The third new twist introduced by the experiment was the emergence of natural behavior . It was not only that the system could dream , but also that wholly new modes  --  images  --  that were never stored in the holographic memory could emerge . The system therefore exhibited a degree of creativity . It thought up answers that had not been preprogrammed into the holographic memory . The creative behavior @ @ @ @ @ @ @ @ @ @ of a few inanimate optics , raises an interesting question about whether human creativity and intuition is somehow connected to our ability to dream . <p> More sophisticated demonstrations that expanded the capabilities of these dynamic holographic neural networks followed Anderson 's initial work . Successive applications began to use these modules as discrete components in a form of optical circuit board , just as transistors and capacitors are combined inside special purpose integrated circuit modules . Miniaturization of the modules also took place and free-space propagation was in some cases replaced with fiber optics , making the systems more compact and economical . <p> Other applications expanded as well . Using the concepts of mode competition and modular design , devices were constructed that could perform as optical spectrum analyzers that could break apart the spectrum of a symphony ; as speckle demultiplexers for multiple data signals transmitted down multi-mode fibers ; as sequence generators that could step through an ordered set of modes or images ; and as optical delay lines that were used in neural networks for voice recognition . <p> The growing number of imaginative demonstrations using @ @ @ @ @ @ @ @ @ @ capabilities of adaptive holograms . These applications operate fundamentally with images that are fully parallel data structures , and that tap directly into the parallel advantage of light . Architecture requires forms with distinct functions that are assembled in a system with complex structure and behavior . The modules are beginning to provide those distinct functions , and their assembly into systems is the beginning of overarching and unifying principles of design  --  the new architecture of light . EVEN PLENTY OF ROOM CAN BE FILLED <p> With such bright potential from these image machines , combined with the imminent arrival of the all-optical Internet , we seem poised in the entryway to a new generation of machines of light . But many of the proof-of-principle demonstrations that I have described are already ancient history . Fifteen years ago , it looked as if the inauguration of such networks into specialized market niches was imminent . The benefits of optics over electronic computers were dear . But as is well known , Moore 's law has pushed the performance of silicon-based computers far beyond where they were in the mid-1980s . @ @ @ @ @ @ @ @ @ @ keep the technology doubling every eighteen months . It is therefore a moving target that optical computing has had difficulty overtaking . That is about to change . Although silicon processor speeds continue to increase along the path ordained by Moore , and gigahertz machines have been introduced into the market , there is a growing bottleneck inside the computer architecture that is strangling the benefits of fast clock speeds . Getting information from cache memory to the processing unit is the bottleneck that has already forced the speed of software bench tests to veer off the exponential slope of Moore 's Law . The processor , though capable of great speed , has to idle while it waits for the information it needs to do its next operation . Some solutions are already being explored by the silicon architects , such as multi-threading , which is a tactic to anticipate what information the processor will need , not for the next operation , but down the road . But even this is only a stopgap . <p> If you project Moore 's law into the future and count how many @ @ @ @ @ @ @ @ @ @ find that by the year 2020 , a transistor should operate on only a single electron . I do not believe we will have large-scale silicon processors operating with single electron transistors by that time . Whole new technologies would need to mature and match the exponential trends of silicon to make that happen . Even though thousands of scientists and engineers are trying hard to make this happen , there are problems facing the electronics industry that are every bit as difficult as the technological problems facing optical computing . Therefore , silicon 's momentum must slow , and the slack will be taken up by optics . The machines of light will face , for the first time in thirty years , a level playing field . And just as light-in-the-box marries the best features of light and electronics , I predict that **27;1321;TOOLONG will combine the strong electron interactions with the parallelism of the image . <p> A tantalizing scenario is emerging that pushes both optics and nano-electronics to their very limits  --  in quantum devices . In the new quantum generation , photons and electrons will collaborate @ @ @ @ @ @ @ @ @ @ ILLUSTRATIONS ( COLOR ) <p> By David Nolte <p> <p> David Nolte is a professor of physics at Purdue University and author of the new book Mind At Light Speed , from which this is excerpted by permission from The Free Press . <p>                                                             