@@432427

 | In the quest for ever faster computers , researchers are turning to the oldest computer design around : The human brain . Though engineers have sped up computer circuits by more than 1,000 times over the past 40 years , they know their efforts are finally reaching the end of the line . Chips ca n't get much smaller , and even electricity has a speed limit . Besides , no amount of tinkering will alter the fact that the fastest conventional computer works on a problem by processing pieces of data one by one ? which is a little like moving a sand dune grain by grain . The circuits of the brain , however , work in parallel ? that is , all at the same time . The new computers work that way , too . At its simplest , a parallel processor consists of a few computers linked together to jointly tackle a problem . But at the other end of the spectrum is a new computer design that more closely mimics the structure of the brain . Called a neural network , @ @ @ @ @ @ @ @ @ @ counterparts of the brain 's nerve cells . Like the human brain , neural networks are not programed but rather taught through examples . Even the makers of large conventional supercomputers are beginning to take parallel processing seriously ? at least in its more modest forms . IBM , Cray Research and AT &T; Bell Laboratories have all recently announced new projects to design and build parallel computers . Boston 's Thinking Machines Corporation is already selling a computer with more than 65,000 processors linked together . " Things are really beginning to take off , " says Gorden Bell , a pioneer in computer design who offers two $1,000 prizes each year for the best research projects in parallel processing . " Research has been going on for decades , but the machines were only toys . Now , they are starting to be used on real problems . " Parallel processing is especially good at those real problems that require keeping track of millions of interacting snippets of data . Examples include predicting the weather , figuring out the distribution of stress on a beam used in construction , @ @ @ @ @ @ @ @ @ @ piece of information . Even on high-tech , superfast ? and superexpensive ? computers , those problems can be time-consuming . Like a fleet of VW 's . A faster and cheaper alternative may be to use less complex computers working in concert , says Bell . " With a Ferrari , you can quickly shuttle 10 people , one at a time , across town , " he says . " But you could also do the job all at once with 10 Volkswagens . " Researchers have followed two basic paths in designing parallel computers . In the " coarse grained " approach , a small number of computers , each typically more powerful than a home computer , are linked together . More recently , researchers have begun to create computers that are " fine grained , " with thousands of much simpler processors tied together . Though it is fairly easy to physically connect a bunch of computers , making them do something useful is not . The biggest obstacle is figuring out how to distribute the problem among the many processors . In coarse-grained computers , @ @ @ @ @ @ @ @ @ @ house . A carpenter might frame a wall while a plumber installs a sink ; so different processors work on different parts of the problem at the same time . One advantage of a coarse-grained system is that , since it has only a few processors , it is easier to adapt a program from the vast store of software already written for conventional computers . Still , some kinds of problems do n't always split up neatly into parallel paths . In the same way that you have to plaster a wall before you can paint it , many problems have a few equations that depend on the solutions to other equations , meaning one set of calculations has to be done before the other . So while the bulk of a problem may be split up among several processors , the  | machine still has to wait around while one of the processors takes care of the stepwise part of the calculation . " When you are running a relay race with a tortoise and a hare , " says Bell , " the tortoise is going to have @ @ @ @ @ @ @ @ @ @ assumed until recently that in any problem at least a half a percent of the work would have to be done step by step . The slowdown from doing that stepwise part of the calculations ? plus the communication congestion that results when the processors trade data back and forth ? was thought to limit the speedup of parallel computers to about 200 times , no matter how many processors were joined together . Researchers at Sandia National Laboratories in Albuquerque , however , recently proved that assumption wrong . Using a 1,024-processor machine , they solved three classic engineering problems more than 1,000 times faster than a single processor working alone . One of the problems , calculating the stress patterns on a construction beam , would have taken a conventional computer 20 years to solve . Sandia 's did it in a week . While the Sandia researchers used the kind of powerful processors found in coarse-grained parallel computers , they achieved their speed breakthrough using a programing strategy more typical of fine-grained machines . Instead of each processor 's doing a different kind of computation , all the @ @ @ @ @ @ @ @ @ @ parts of the data . That 's like all the workers on a house teaming up to do the carpentry , then moving on as a team to the plastering , and so on . Moving mountains of data . Sandia 's approach may be applicable to many similar problems that involve handling vast amounts of data . One such problem is creating a topological map of a terrain by comparing stereo images taken overhead by aircraft . On the Thinking Machines Connection Machine , for example , each of its 65,536 processors is responsible for a tiny piece of the map . The processors ? each no more powerful than the computer chip in a typical videogame ? work simultaneously to put together the entire drawing . The Connection Machine can make 7,000 contour maps in an hour ; a conventional machine makes 15 maps per hour . Likewise , a model of air flowing over an airplane wing can be generated by  | assigning each processor the role of representing a particle of air , and letting it interact with its neighboring processors in the same way that neighboring @ @ @ @ @ @ @ @ @ @ The most radical entry into the parallel computing world , neural networks , takes the fine-grain approach to its limit . The machines , which for the most part now exist only as experimental models in research labs , do n't have conventional processors at all . They consist of networks of extremely simple devices , each of which adds up incoming signals from the others and makes a decision to send out a signal of its own ? a process analogous to how neurons operate in the brain . Rather than being programed , these neural networks are taught to solve problems through examples . The machine is given a series of handwritten characters , for instance , and told which letter of the alphabet each character represents . As it runs through the examples , the network adjusts the connections between its neurons to produce a better and better match between the handwritten character and the proper letter of the alphabet . Eventually , it learns to make the association on its own . Other experimental networks have learned to read English text aloud in a few hours and @ @ @ @ @ @ @ @ @ @ many neurons acting all at once , they are ideal for tasks like visual pattern recognition . Their ability to do rudimentary learning on their own also gives them the potential to perform complex tasks ? such as recognizing spoken words ? where artificial-intelligence researchers still do n't have a clear idea how to write a computer program to do it . But since all the neurons work at once , the networks ca n't do the types of sequential processing that an ordinary computer does . Eventually , say researchers , neural networks may have to be joined with other neural networks to do stepwise operations . In a way , then , neural networks may bring the approaches to designing the computers of the future around full circle . Instead of hooking up powerful serial processors into a large machine that can also do parallel computation , designers of future computers may join together very weak ? but massively parallel ? processors into a large machine that can also do serial processing . That may sound like a bizarre type of computer , but in fact it 's probably @ @ @ @ @ @ @ @ @ @ that 's the basic design of the biological computer you are using to read this sentence . The surge of interest in parallel processing has revitalized another neglected corner of computer research : Computing with light . Computer engineers have long been tantalized by the prospect of using light rays instead of electricity in their machines . Light waves do n't need wires to travel in , so their paths can be packed closer together than in circuits that use electricity . What 's more , since light rays can be shunted in many directions at once , it makes it easier for the various parts of an optical computer to communicate with each other . But lenses and mirrors lack the precision of electronics ? and researchers have lacked an obvious incentive to overcome the technical obstacles to improving them . Now , the promise of parallel processing may change that . The potential to use light for densely packed , highly communicative circuits is a perfect fit for the needs of parallel processing . Most optical computers use a laser beam to carry information . Mirrors and holograms are @ @ @ @ @ @ @ @ @ @ as transistors in conventional computer chips control the flow of electric current . Because they process visual images directly , parallel optical computers are well suited to recognizing complex structures such as faces , trees and fingerprints , says Demetri Psaltis , an optics researcher from the California Institute of Technology . Since there is no compact mathematical way of describing something like a fingerprint , a computer has to store the entire image . To match a fingerprint , a conventional computer would then have to check through its store of images one by one ? a burdensome task . A parallel optical computer , on the other hand , has the potential to search through all the images at once . Psaltis has constructed an optical computer that can categorize images of different types of trees . It can also recognize a partial image of a person 's face as one of several stored in its memory . To match an image of a tree to one in memory , Psaltis 's computer takes light beams from the input image and directs them through a hologram that has images @ @ @ @ @ @ @ @ @ @ light is then sent to output sensors , which in turn direct the light through the hologram again in a feedback loop . As the light beams zip around and around through the hologram , the stored memory that best matches the input image is reinforced . That matching output image is then projected on a video monitor . It will be a long time before an optical computer makes it into the local hardware store . Most researchers think that the first optical computers will be special-purpose machines ? a fingerprint recognizer , for example ? and then other , more general machines will be developed .                     