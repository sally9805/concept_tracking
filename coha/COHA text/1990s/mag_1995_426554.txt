@@426554

Section : Warning Labels Required ? <p> For the past 16 years the National Highway Traffic Safety Administration ( NHTSA ) has crash tested about 30 new cars per year in a program to publicize the relative crashworthiness of these cars . This New Car Assessment Program , or NCAP , uses a somewhat more violent test than that required for compliance with federal safety standards . It involves crashing a car head-on at 35 m.p.h. into a fixed , rigid barrier ( compliance tests call for 30 m.p.h. crashes ) . From forces measured on fully-belted test dummies , scores predictive of the chance for serious head , chest , and leg injuries then are published . These scores provide the basis for comparing tested cars . Presently , less-precise " star " ratings substitute for the scores in results released to the public . <p> The usefulness of NCAP results has been questionable from the start  --  which is why CR does not publish them . Even so , they have received wide use and publicity by consumer groups and magazines . Most present the necessary @ @ @ @ @ @ @ @ @ @ comparisons can be made only between cars that weigh within 500 pounds of each other . Yet the reliability of the scores rarely receives adequate discussion , lending them a sense of validity that has not been proved . Even government consumer publications gloss over this . <p> A new report* from the General Accounting Office ( GAO ) , excerpted below , questions this reliability . While the investigators conclude that NCAP probably has helped improve crashworthiness overall , they find no evidence that individual scores are comparable or even very indicative of real-world safety . Large differences between cars may disappear or reverse when tested again . Thus , consumers actually may be misled by the scores into purchasing less safe cars , a fact that GAO says should be released with NCAP results  --  in essence , a warning label for a consumer safety program .  --  Ed . <p> Recent trends in test results indicate that , first , the probability of sustaining a serious injury , as it is measured by the NCAP and compliance tests , has decreased substantially since the inception of these test @ @ @ @ @ @ @ @ @ @ automobile models has shrunk , indicating that cars marketed in the United States have become more uniformly crashworthy . Some of this improvement , we concluded , could appropriately be attributed to DOT ( Department of Transportation ) crash-test programs . . . . <p> Second , the reliability , that is , the consistency of results derived from NCAP tests is questionable . Existing evidence suggests that large differences between crash-test scores most likely reflect true differences between vehicles . However , we can not be sure that even moderately large score differences between two vehicles might not disappear or be reversed if they were tested again . This is because , in general , only one unit of a specific vehicle line is tested , and this is not enough to say with confidence that the results are indicative of how other units of that same vehicle line would perform . The new star rating system incorporated by NCAP in recent years , a system that places vehicles into one of five categories on the basis of potential injury risk , could exacerbate this problem in some cases . @ @ @ @ @ @ @ @ @ @ to predict a vehicle 's occupant protection in real-world crashes is limited . By their nature , NCAP crash-test results can be validly applied only to frontal collisions , which account for slightly more than half of all injury-producing accidents . We found a statistically significant relationship between fatality rates and NCAP predicted injury risk ; however , this relationship derives from the high fatality rates associated with the poorest performers in NCAP . . . . <p> Autos More Crashworthy . In NCAP crashes , nearly all cars now meet the head and chest injury standards of the compliance tests , although they are 36% more violent than compliance crashes . The average probability of sustaining a serious injury in a 35 mile-per-hour crash as measured by NCAP has declined from over 0.5 in 1980 to less than 0.2 in 1993 . Differences among the crash-worthiness scores of vehicles tested in this program have experienced similar declines . The introduction of air bags has contributed significantly to this improvement . <p> A causal linkage between improved crash-test scores and declining highway fatalities can not be asserted with certainty because of @ @ @ @ @ @ @ @ @ @ increased emphasis on traffic accident and injury prevention over the past decade . Nonetheless , it seems reasonable to conclude that manufacturers ' successful efforts to improve their products ' performance in NHTSA crash tests , particularly in NCAP , have contributed to improved occupant protection in real-world crashes , although we were unable to quantify that contribution . These improvements to performance have derived from a variety of efforts , with two examples being modernized manufacturing techniques and an increased emphasis on safety systems and designs . . . . <p> Questionable Reliability . To determine whether the result of any test is reliable , consistent results must be obtained through repeated trials of a specified procedure . In the case of crash tests , this means that consistent results of repeated tests of a specific vehicle model are required . This is particularly crucial when comparing the safety ratings of different vehicles . Both the NCAP and compliance programs generally conduct only one trial of a specific vehicle model ; thus , insufficient data exist to accurately define the reliability of crash-test results . That is , the ability @ @ @ @ @ @ @ @ @ @ 's receiving similar scores if tested again is low . <p> We found only two sources of information on which to assess the reliability of crash-test results : a study conducted by NHTSA in 1984 , which examined the variations in test results of 12 consecutively manufactured Chevrolet Citations , and our own analysis of the differences between results for vehicle models tested in NCAP and the results for those vehicles in corresponding tests conducted by automobile manufacturers . Our analysis of the data derived from NHTSA 's 1984 study revealed wide variations in the head injury criterion ( HIC ) results , the measurements taken to assess potential skeletal head injuries . Although NHTSA ascribed the variation in results to a number of sources , including the test itself , it failed to discuss the implications of the combined effect of these sources on crash-test results ; namely , that even within a specific vehicle line , the result of one test may not be indicative of the model 's performance from trial to trial , and large differences in the resultant HIC may occur . <p> We also examined @ @ @ @ @ @ @ @ @ @ tests provided to us by NCAP officials . The tests conducted by the manufacturers essentially duplicated the NCAP test procedures . We compared the results of the two tests using the star rating system recently developed by NCAP , hypothesizing that if the manufacturer test was considered a second trial for a model line , its results should be consistent with the NCAP , or first trial . The star rating system ranks cars from one to five stars , with five stars being the best rating , or safest car , and one star being the worst rating , or least safe car . These ratings are based on the risks of serious injury for vehicles , which are calculated from the head-injury criterion and chest-acceleration scores from NCAP tests . <p> We found that in only about one-half of the paired comparisons would NCAP- and manufacturer-tested vehicles have received the same star rating . In 32% of the comparisons , the results of the second trial would have changed by one star , while in 8% of the cases , the ratings would have changed by two or more @ @ @ @ @ @ @ @ @ @ ( the base unit categorized into the five star ratings ) derived from the manufacturer and NCAP data , we found that each star category was associated with a wide band in which the resultant risk scores of subsequent tests might fall . For example , the results of a second test of a vehicle rated as four stars by its first test could fall between five stars and two stars . <p> The analyses described above are based on the only two sources of information we could find . The quantity of data in each analysis was not enough for us to fully quantify the reliability of crash-test results ; however , we were able to determine that NCAP scores , whether reported in raw HIC and chest-acceleration scores or as categories of injury probability , have associated levels of imprecision and that seemingly large differences in crash-test results may not necessarily reflect true differences in a vehicle 's safety potential . By not properly defining and publishing the degree of reliability , consumers may be misled into purchasing a vehicle purported to be more crashworthy than another when , @ @ @ @ @ @ @ @ @ @ or even less safe , than the comparison vehicle . <p> Results vs . Real World Fatality Rates . Since NCAP crash tests are designed to simulate full-frontal collisions , we restricted our analysis to those types of crashes and found the results of NCAP crash tests are generally reflected in real-world fatality rates . That is , on the whole , a statistically significant relationship exists between real-world highway fatality rates associated with vehicles tested in the NCAP program and their scores in crash tests . However , we concluded that this relationship derives mainly from the high fatality rates of vehicles with the worst NCAP scores . When we divided vehicles into NCAP score quintiles  --  that is , placed the vehicles into one of five 20-percentile categories based on their location in the distribution of NCAP results  --  we found that the quintile with the worst NCAP scores ( those vehicles in the highest 20-percentile category ) had significantly higher fatality rates than the remaining 80% of NCAP tested vehicles . The remaining four quintile categories , however , had associated fatality rates that were not significantly different @ @ @ @ @ @ @ @ @ @ the mean risk of injury in frontal crashes , as measured by NHTSA crash tests , has declined and indeed has mirrored a similar trend in the annual number of highway fatalities . . . . <p> These trends in the mean score of crash tests , however , do not necessarily suggest that individual vehicles have well-defined levels of safety , nor do they suggest that relative rankings of two vehicles would be the same if subsequent trials were conducted . They also do not suggest that differing results are reflected in data derived from real-world traffic collisions . Indeed , only the poorest performers in NCAP had associated fatality rates that were significantly different from other NCAP vehicles . <p> * Reliability and Validity of DOT Crash Tests , May 1995 Additional Comments On Crash Ratings . . . <p> Congress told NHTSA in 1992 to provide the public with " easily understandable " safety performance information on vehicles . The agency previously published numerical injury scores indicating the likelihood of head , chest , and femur injuries . These scores are still available on request , but now @ @ @ @ @ @ @ @ @ @ ) injury data into a single rating of one to five stars that NHTSA claims represents the level of a car 's protection in a head-on crash . <p> " The star system oversimplifies something that inherently is n't simple , " counters Institute President Brian O'Neill . The Institute has been critical of the star system for translating relatively small differences in injury criteria into what look like major differences among vehicles  --  three stars instead of four , for example . Stars also encourage consumers to erroneously compare scores across car size and weight classes . See " Calling All Consumers , " CR , April 1994 . <p> Institute Senior Vice President Adrian Lurid raises another issue . " Any vehicle with a head injury criterion somewhat higher than 1,200 receives a single star , " he points out . There 's no further distinction among criteria higher ( worse ) than 1,500 or even 2,000 , although " it 's the very high scores , not those around 1,000 to 1,200 , that have been clearly associated with higher fatality likelihood . " <p> Conversely , vehicles @ @ @ @ @ @ @ @ @ @ suggesting these vehicles are five times safer than those with one star . " In fact , there 's no statistical evidence that distinctions in this range are systematically related to crashworthiness at all , let alone to the extent suggested by the star system , " Lund explains .  --  Insurance Institute for Highway Safety . <p> <p> NHTSA implemented a revised rating system providing a combined relative rank in the form of one to five stars , with five stars being the best rating . The combined system more directly addresses consumer concerns regarding the potential for injury regardless of type and eliminates the numerical standings and their implied precision . . . . In addition , the implementation of the star system for displaying crash-test results will eliminate concerns related to minor potential variations .  --  National Highway Traffic Safety Administration <p> <p> Our findings provide detail to support the conclusion . . . that a reporting system can be no more reliable than the scoring system on which it is based . <p> We disagree that the new star rating system " eliminates . . . the @ @ @ @ @ @ @ @ @ @ that some cars with nonsignificant differences in scores would end up in the same category under the new system , and thus correctly be presented to the public as roughly equal in crashworthiness . However , it is also true that other cars with nonsignificant score differences could be placed in different categories , a scoring artifact that incorrectly implies substantial differences in the relative levels of crash protection provided by the vehicles . <p>  --  GAO response to NHTSA comments <p>                     